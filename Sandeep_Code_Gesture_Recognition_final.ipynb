{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project - Gesture Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Imagine you are working as a data scientist at a home electronics company which manufactures state of the art smart televisions. You want to develop a cool feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without using a remote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition Feature for Smart TV\n",
    "\n",
    "## Problem Statement\n",
    "As part of our ongoing efforts to innovate in the home electronics space, our team at the cutting-edge smart television manufacturing company is embarking on developing an interactive feature for our smart TVs. This feature aims to recognize five distinct gestures performed by the user, enabling control of the TV without the necessity of a physical remote. This gesture-based control system promises to enhance user interaction by offering a more intuitive and seamless experience.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "### Generator\n",
    "- The generator is tasked with processing a batch of video inputs flawlessly. It must execute steps such as cropping, resizing, and normalization accurately, ensuring that the videos are properly formatted for model training and inference.\n",
    "\n",
    "### Model Development\n",
    "- The core of this project lies in developing a robust model capable of recognizing gestures with high accuracy while maintaining a minimal number of parameters. The efficiency of the model is paramount, as we aim to achieve low inference (prediction) times suitable for a responsive user interface on the smart TV.\n",
    "  - **Initial Approach**: Begin with training the model on a smaller dataset to gauge its performance and identify potential areas for improvement.\n",
    "  - **Progression Strategy**: Gradually increase the dataset size, fine-tuning and adjusting the model based on initial insights to enhance its accuracy and efficiency.\n",
    "\n",
    "### Write-Up\n",
    "- A comprehensive write-up will document the journey towards the final model selection. It will detail:\n",
    "  - **Base Model Choice**: The rationale behind selecting the initial model as our starting point, considering its relevance and potential for the task at hand.\n",
    "  - **Modifications and Experiments**: A chronicle of the modifications, experiments, and metrics considered throughout the project to refine and perfect the model.\n",
    "  - **Final Model Selection**: The process and reasoning for choosing the final model, including discussions on the balance between model accuracy and the number of parameters, to ensure an optimal solution for real-time gesture recognition.\n",
    "\n",
    "This project represents a significant leap towards creating more engaging and user-friendly smart TV experiences, leveraging advanced gesture recognition technology to set new standards in the industry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "import datetime\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "import imageio\n",
    "import imageio.v2 as imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 10:09:14.531005: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data path: /Users/smalagi/Desktop/Project_data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/Users/smalagi/Desktop/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/Users/smalagi/Desktop/Project_data/val.csv').readlines())\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 30 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,x)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    temp = resize(image,(120,120))\n",
    "                    temp = temp/127.5-1 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2]) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = resize(image,(120,120))\n",
    "                    temp = temp/127.5-1 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/Users/smalagi/Desktop/Project_data/train'\n",
    "val_path = '/Users/smalagi/Desktop/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, GRU, Flatten, TimeDistributed, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "#write your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model a\n",
    "model_a = Sequential()\n",
    "\n",
    "model_a.add(Conv3D(8, #number of filters \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=(30, 120, 120, 3),\n",
    "                 padding='same'))\n",
    "model_a.add(BatchNormalization())\n",
    "model_a.add(Activation('relu'))\n",
    "\n",
    "model_a.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_a.add(Conv3D(16, #Number of filters, \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model_a.add(BatchNormalization())\n",
    "model_a.add(Activation('relu'))\n",
    "\n",
    "model_a.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_a.add(Conv3D(32, #Number of filters \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model_a.add(BatchNormalization())\n",
    "model_a.add(Activation('relu'))\n",
    "\n",
    "model_a.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_a.add(Conv3D(64, #Number pf filters \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model_a.add(BatchNormalization())\n",
    "model_a.add(Activation('relu'))\n",
    "\n",
    "model_a.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model_a.add(Flatten())\n",
    "\n",
    "model_a.add(Dense(1000, activation='relu'))\n",
    "model_a.add(Dropout(0.5))\n",
    "\n",
    "model_a.add(Dense(500, activation='relu'))\n",
    "model_a.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model_a.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 30, 120, 120, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 30, 120, 120, 8)   32        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3  (None, 15, 60, 60, 8)     0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 15, 60, 60, 16)    3472      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 15, 60, 60, 16)    64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 15, 60, 60, 16)    0         \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPoolin  (None, 7, 30, 30, 16)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 7, 30, 30, 32)     4640      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 7, 30, 30, 32)     128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 7, 30, 30, 32)     0         \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPoolin  (None, 3, 15, 15, 32)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 3, 15, 15, 64)     18496     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 3, 15, 15, 64)     256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 3, 15, 15, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPoolin  (None, 1, 7, 7, 64)       0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              3137000   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3667749 (13.99 MB)\n",
      "Trainable params: 3667509 (13.99 MB)\n",
      "Non-trainable params: 240 (960.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = tf.keras.optimizers.Adam(learning_rate=0.001) # Updated to use `learning_rate` instead of `lr`\n",
    "model_a.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print(model_a.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "\n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "# Updated ModelCheckpoint to use `save_freq='epoch'`\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # ReduceLROnPlateau setup remains unchanged\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit` method to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /Users/smalagi/Desktop/Project_data/train ; batch size = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/2313827601.py:17: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/67 [===========================>..] - ETA: 14s - loss: 4.4204 - categorical_accuracy: 0.2922Batch:  67 Index: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/2313827601.py:42: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - ETA: 0s - loss: 4.3440 - categorical_accuracy: 0.2911Source path =  /Users/smalagi/Desktop/Project_data/val ; batch size = 10\n",
      "\n",
      "Epoch 1: saving model to model_init_2024-02-0310_09_19.822402/model-00001-4.34400-0.29110-1.30156-0.42000.h5\n",
      "67/67 [==============================] - 362s 5s/step - loss: 4.3440 - categorical_accuracy: 0.2911 - val_loss: 1.3016 - val_categorical_accuracy: 0.4200 - lr: 0.0010\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smalagi/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - ETA: 0s - loss: 2.4085 - categorical_accuracy: 0.2836\n",
      "Epoch 2: saving model to model_init_2024-02-0310_09_19.822402/model-00002-2.40846-0.28358-1.31336-0.37000.h5\n",
      "67/67 [==============================] - 147s 2s/step - loss: 2.4085 - categorical_accuracy: 0.2836 - val_loss: 1.3134 - val_categorical_accuracy: 0.3700 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.9519 - categorical_accuracy: 0.2985\n",
      "Epoch 3: saving model to model_init_2024-02-0310_09_19.822402/model-00003-1.95192-0.29851-1.47133-0.31000.h5\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "67/67 [==============================] - 153s 2s/step - loss: 1.9519 - categorical_accuracy: 0.2985 - val_loss: 1.4713 - val_categorical_accuracy: 0.3100 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5154 - categorical_accuracy: 0.3383\n",
      "Epoch 4: saving model to model_init_2024-02-0310_09_19.822402/model-00004-1.51540-0.33831-1.45652-0.31000.h5\n",
      "67/67 [==============================] - 148s 2s/step - loss: 1.5154 - categorical_accuracy: 0.3383 - val_loss: 1.4565 - val_categorical_accuracy: 0.3100 - lr: 5.0000e-04\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4807 - categorical_accuracy: 0.3632\n",
      "Epoch 5: saving model to model_init_2024-02-0310_09_19.822402/model-00005-1.48070-0.36318-1.32230-0.44000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "67/67 [==============================] - 148s 2s/step - loss: 1.4807 - categorical_accuracy: 0.3632 - val_loss: 1.3223 - val_categorical_accuracy: 0.4400 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4223 - categorical_accuracy: 0.3333\n",
      "Epoch 6: saving model to model_init_2024-02-0310_09_19.822402/model-00006-1.42233-0.33333-1.22319-0.48000.h5\n",
      "67/67 [==============================] - 146s 2s/step - loss: 1.4223 - categorical_accuracy: 0.3333 - val_loss: 1.2232 - val_categorical_accuracy: 0.4800 - lr: 2.5000e-04\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.3598 - categorical_accuracy: 0.3831\n",
      "Epoch 7: saving model to model_init_2024-02-0310_09_19.822402/model-00007-1.35984-0.38308-1.21327-0.55000.h5\n",
      "67/67 [==============================] - 148s 2s/step - loss: 1.3598 - categorical_accuracy: 0.3831 - val_loss: 1.2133 - val_categorical_accuracy: 0.5500 - lr: 2.5000e-04\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.3400 - categorical_accuracy: 0.3781\n",
      "Epoch 8: saving model to model_init_2024-02-0310_09_19.822402/model-00008-1.33995-0.37811-1.17986-0.45000.h5\n",
      "67/67 [==============================] - 147s 2s/step - loss: 1.3400 - categorical_accuracy: 0.3781 - val_loss: 1.1799 - val_categorical_accuracy: 0.4500 - lr: 2.5000e-04\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.2664 - categorical_accuracy: 0.4179\n",
      "Epoch 9: saving model to model_init_2024-02-0310_09_19.822402/model-00009-1.26644-0.41791-1.22539-0.51000.h5\n",
      "67/67 [==============================] - 137s 2s/step - loss: 1.2664 - categorical_accuracy: 0.4179 - val_loss: 1.2254 - val_categorical_accuracy: 0.5100 - lr: 2.5000e-04\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.3029 - categorical_accuracy: 0.4527\n",
      "Epoch 10: saving model to model_init_2024-02-0310_09_19.822402/model-00010-1.30292-0.45274-1.19570-0.44000.h5\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "67/67 [==============================] - 131s 2s/step - loss: 1.3029 - categorical_accuracy: 0.4527 - val_loss: 1.1957 - val_categorical_accuracy: 0.4400 - lr: 2.5000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff4f16072b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_a.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "            callbacks=callbacks_list, validation_data=val_generator, \n",
    "            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 5 #left swipe, right swipe, thumbs up, thumbs down, stop\n",
    "channel = 3\n",
    "x = 30 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "\n",
    "def generator_ex(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,x)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,classes)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    temp = resize(image,(y,z))\n",
    "                    #Converting to gray scale\n",
    "                    temp = temp.mean(axis=-1,keepdims=1) \n",
    "                    temp = temp/127.5-1 #Normalize data\n",
    "                    batch_data[folder,idx] = temp #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                \n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,classes)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = resize(image,(y,z))\n",
    "                    #Converting to gray scale\n",
    "                    temp = temp.mean(axis=-1,keepdims=1) \n",
    "                    temp = temp/127.5-1 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx] = temp\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_4 (Conv3D)           (None, 30, 120, 120, 32   2624      \n",
      "                             )                                   \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 30, 120, 120, 32   0         \n",
      "                             )                                   \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 30, 120, 120, 32   27680     \n",
      "                             )                                   \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 30, 120, 120, 32   0         \n",
      "                             )                                   \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPoolin  (None, 10, 40, 40, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 10, 40, 40, 32)    0         \n",
      "                                                                 \n",
      " conv3d_6 (Conv3D)           (None, 10, 40, 40, 64)    55360     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 10, 40, 40, 64)    0         \n",
      "                                                                 \n",
      " conv3d_7 (Conv3D)           (None, 10, 40, 40, 64)    110656    \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 10, 40, 40, 64)    0         \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPoolin  (None, 4, 14, 14, 64)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 4, 14, 14, 64)     0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 50176)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               25690624  \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25889509 (98.76 MB)\n",
      "Trainable params: 25889509 (98.76 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model b\n",
    "model_b = Sequential()\n",
    "model_b.add(Conv3D(32, kernel_size=(3, 3, 3), input_shape=(x,y,z,channel), padding='same'))\n",
    "model_b.add(Activation('relu'))\n",
    "model_b.add(Conv3D(32, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_b.add(Activation('relu'))\n",
    "model_b.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model_b.add(Dropout(0.25))\n",
    "\n",
    "model_b.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_b.add(Activation('relu'))\n",
    "model_b.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_b.add(Activation('relu'))\n",
    "model_b.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model_b.add(Dropout(0.25))\n",
    "\n",
    "model_b.add(Flatten())\n",
    "model_b.add(Dense(512, activation='relu'))\n",
    "model_b.add(Dropout(0.5))\n",
    "model_b.add(Dense(classes, activation='softmax'))\n",
    "\n",
    "model_b.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator_ex(train_path, train_doc, batch_size)\n",
    "val_generator = generator_ex(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /Users/smalagi/Desktop/Project_data/train ; batch size = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:19: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/67 [===========================>..] - ETA: 1:13 - loss: 1.9284 - categorical_accuracy: 0.2047Batch:  67 Index: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:44: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - ETA: 0s - loss: 1.9175 - categorical_accuracy: 0.2006 Source path =  /Users/smalagi/Desktop/Project_data/val ; batch size = 10\n",
      "\n",
      "Epoch 1: saving model to model_init_2024-02-0310_09_19.822402/model-00001-1.91754-0.20060-1.60848-0.23000.h5\n",
      "67/67 [==============================] - 1674s 25s/step - loss: 1.9175 - categorical_accuracy: 0.2006 - val_loss: 1.6085 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6109 - categorical_accuracy: 0.1891\n",
      "Epoch 2: saving model to model_init_2024-02-0310_09_19.822402/model-00002-1.61095-0.18905-1.60883-0.17000.h5\n",
      "67/67 [==============================] - 562s 8s/step - loss: 1.6109 - categorical_accuracy: 0.1891 - val_loss: 1.6088 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6104 - categorical_accuracy: 0.1791\n",
      "Epoch 3: saving model to model_init_2024-02-0310_09_19.822402/model-00003-1.61040-0.17910-1.61026-0.14000.h5\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "67/67 [==============================] - 555s 8s/step - loss: 1.6104 - categorical_accuracy: 0.1791 - val_loss: 1.6103 - val_categorical_accuracy: 0.1400 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6102 - categorical_accuracy: 0.1791\n",
      "Epoch 4: saving model to model_init_2024-02-0310_09_19.822402/model-00004-1.61019-0.17910-1.60856-0.26000.h5\n",
      "67/67 [==============================] - 547s 8s/step - loss: 1.6102 - categorical_accuracy: 0.1791 - val_loss: 1.6086 - val_categorical_accuracy: 0.2600 - lr: 5.0000e-04\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6093 - categorical_accuracy: 0.2090\n",
      "Epoch 5: saving model to model_init_2024-02-0310_09_19.822402/model-00005-1.60932-0.20896-1.60856-0.20000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "67/67 [==============================] - 551s 8s/step - loss: 1.6093 - categorical_accuracy: 0.2090 - val_loss: 1.6086 - val_categorical_accuracy: 0.2000 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6104 - categorical_accuracy: 0.1940\n",
      "Epoch 6: saving model to model_init_2024-02-0310_09_19.822402/model-00006-1.61040-0.19403-1.60899-0.20000.h5\n",
      "67/67 [==============================] - 568s 8s/step - loss: 1.6104 - categorical_accuracy: 0.1940 - val_loss: 1.6090 - val_categorical_accuracy: 0.2000 - lr: 2.5000e-04\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6088 - categorical_accuracy: 0.2388\n",
      "Epoch 7: saving model to model_init_2024-02-0310_09_19.822402/model-00007-1.60884-0.23881-1.60686-0.28000.h5\n",
      "67/67 [==============================] - 581s 9s/step - loss: 1.6088 - categorical_accuracy: 0.2388 - val_loss: 1.6069 - val_categorical_accuracy: 0.2800 - lr: 2.5000e-04\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6101 - categorical_accuracy: 0.1592\n",
      "Epoch 8: saving model to model_init_2024-02-0310_09_19.822402/model-00008-1.61007-0.15920-1.60866-0.19000.h5\n",
      "67/67 [==============================] - 565s 8s/step - loss: 1.6101 - categorical_accuracy: 0.1592 - val_loss: 1.6087 - val_categorical_accuracy: 0.1900 - lr: 2.5000e-04\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6097 - categorical_accuracy: 0.1542\n",
      "Epoch 9: saving model to model_init_2024-02-0310_09_19.822402/model-00009-1.60966-0.15423-1.60816-0.25000.h5\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "67/67 [==============================] - 564s 8s/step - loss: 1.6097 - categorical_accuracy: 0.1542 - val_loss: 1.6082 - val_categorical_accuracy: 0.2500 - lr: 2.5000e-04\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6095 - categorical_accuracy: 0.1741\n",
      "Epoch 10: saving model to model_init_2024-02-0310_09_19.822402/model-00010-1.60949-0.17413-1.60980-0.21000.h5\n",
      "67/67 [==============================] - 569s 9s/step - loss: 1.6095 - categorical_accuracy: 0.1741 - val_loss: 1.6098 - val_categorical_accuracy: 0.2100 - lr: 1.2500e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff5012803d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_8 (Conv3D)           (None, 30, 60, 60, 32)    2624      \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 30, 60, 60, 32)    0         \n",
      "                                                                 \n",
      " conv3d_9 (Conv3D)           (None, 30, 60, 60, 32)    27680     \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 30, 60, 60, 32)    0         \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPoolin  (None, 10, 20, 20, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 10, 20, 20, 32)    0         \n",
      "                                                                 \n",
      " conv3d_10 (Conv3D)          (None, 10, 20, 20, 64)    55360     \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 10, 20, 20, 64)    0         \n",
      "                                                                 \n",
      " conv3d_11 (Conv3D)          (None, 10, 20, 20, 64)    110656    \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 10, 20, 20, 64)    0         \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPoolin  (None, 4, 7, 7, 64)       0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 4, 7, 7, 64)       0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               6423040   \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6621925 (25.26 MB)\n",
      "Trainable params: 6621925 (25.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = 30 # number of frames\n",
    "y = 60 # image width\n",
    "z = 60 # image height\n",
    "\n",
    "# Define model b\n",
    "model_b = Sequential()\n",
    "model_b.add(Conv3D(32, kernel_size=(3, 3, 3), input_shape=(x,y,z,channel), padding='same'))\n",
    "model_b.add(Activation('relu'))\n",
    "model_b.add(Conv3D(32, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_b.add(Activation('relu'))\n",
    "model_b.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model_b.add(Dropout(0.25))\n",
    "\n",
    "model_b.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_b.add(Activation('relu'))\n",
    "model_b.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_b.add(Activation('relu'))\n",
    "model_b.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model_b.add(Dropout(0.25))\n",
    "\n",
    "model_b.add(Flatten())\n",
    "model_b.add(Dense(512, activation='relu'))\n",
    "model_b.add(Dropout(0.5))\n",
    "model_b.add(Dense(classes, activation='softmax'))\n",
    "\n",
    "model_b.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator_ex(train_path, train_doc, batch_size)\n",
    "val_generator = generator_ex(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /Users/smalagi/Desktop/Project_data/train ; batch size = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:19: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/67 [===========================>..] - ETA: 17s - loss: 1.6371 - categorical_accuracy: 0.2172Batch:  67 Index: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:44: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - ETA: 0s - loss: 1.6359 - categorical_accuracy: 0.2232Source path =  /Users/smalagi/Desktop/Project_data/val ; batch size = 10\n",
      "\n",
      "Epoch 1: saving model to model_init_2024-02-0310_09_19.822402/model-00001-1.63590-0.22323-1.60568-0.24000.h5\n",
      "67/67 [==============================] - 413s 6s/step - loss: 1.6359 - categorical_accuracy: 0.2232 - val_loss: 1.6057 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5571 - categorical_accuracy: 0.2687\n",
      "Epoch 2: saving model to model_init_2024-02-0310_09_19.822402/model-00002-1.55708-0.26866-1.59051-0.25000.h5\n",
      "67/67 [==============================] - 151s 2s/step - loss: 1.5571 - categorical_accuracy: 0.2687 - val_loss: 1.5905 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6391 - categorical_accuracy: 0.1940\n",
      "Epoch 3: saving model to model_init_2024-02-0310_09_19.822402/model-00003-1.63913-0.19403-1.62739-0.24000.h5\n",
      "67/67 [==============================] - 146s 2s/step - loss: 1.6391 - categorical_accuracy: 0.1940 - val_loss: 1.6274 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5167 - categorical_accuracy: 0.3085\n",
      "Epoch 4: saving model to model_init_2024-02-0310_09_19.822402/model-00004-1.51674-0.30846-1.47310-0.37000.h5\n",
      "67/67 [==============================] - 149s 2s/step - loss: 1.5167 - categorical_accuracy: 0.3085 - val_loss: 1.4731 - val_categorical_accuracy: 0.3700 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4433 - categorical_accuracy: 0.3632\n",
      "Epoch 5: saving model to model_init_2024-02-0310_09_19.822402/model-00005-1.44328-0.36318-1.28698-0.43000.h5\n",
      "67/67 [==============================] - 146s 2s/step - loss: 1.4433 - categorical_accuracy: 0.3632 - val_loss: 1.2870 - val_categorical_accuracy: 0.4300 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.3695 - categorical_accuracy: 0.3333\n",
      "Epoch 6: saving model to model_init_2024-02-0310_09_19.822402/model-00006-1.36945-0.33333-1.48651-0.33000.h5\n",
      "67/67 [==============================] - 148s 2s/step - loss: 1.3695 - categorical_accuracy: 0.3333 - val_loss: 1.4865 - val_categorical_accuracy: 0.3300 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.3930 - categorical_accuracy: 0.3781\n",
      "Epoch 7: saving model to model_init_2024-02-0310_09_19.822402/model-00007-1.39305-0.37811-1.28387-0.49000.h5\n",
      "67/67 [==============================] - 149s 2s/step - loss: 1.3930 - categorical_accuracy: 0.3781 - val_loss: 1.2839 - val_categorical_accuracy: 0.4900 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.2888 - categorical_accuracy: 0.4378\n",
      "Epoch 8: saving model to model_init_2024-02-0310_09_19.822402/model-00008-1.28880-0.43781-1.18879-0.41000.h5\n",
      "67/67 [==============================] - 151s 2s/step - loss: 1.2888 - categorical_accuracy: 0.4378 - val_loss: 1.1888 - val_categorical_accuracy: 0.4100 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.2493 - categorical_accuracy: 0.4876\n",
      "Epoch 9: saving model to model_init_2024-02-0310_09_19.822402/model-00009-1.24926-0.48756-1.35747-0.32000.h5\n",
      "67/67 [==============================] - 149s 2s/step - loss: 1.2493 - categorical_accuracy: 0.4876 - val_loss: 1.3575 - val_categorical_accuracy: 0.3200 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.3237 - categorical_accuracy: 0.3831\n",
      "Epoch 10: saving model to model_init_2024-02-0310_09_19.822402/model-00010-1.32371-0.38308-1.32378-0.34000.h5\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "67/67 [==============================] - 149s 2s/step - loss: 1.3237 - categorical_accuracy: 0.3831 - val_loss: 1.3238 - val_categorical_accuracy: 0.3400 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff5012b9600>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator_ex(train_path, train_doc, 20)\n",
    "val_generator = generator_ex(val_path, val_doc, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /Users/smalagi/Desktop/Project_data/train ; batch size = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:19: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31/67 [============>.................] - ETA: 7:03 - loss: 1.0987 - categorical_accuracy: 0.5387Batch:  34 Index: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:44: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - ETA: 0s - loss: 1.0858 - categorical_accuracy: 0.5420Source path =  /Users/smalagi/Desktop/Project_data/val ; batch size = 20\n",
      "\n",
      "Epoch 1: saving model to model_init_2024-02-0310_09_19.822402/model-00001-1.08583-0.54199-1.21729-0.52000.h5\n",
      "67/67 [==============================] - 515s 8s/step - loss: 1.0858 - categorical_accuracy: 0.5420 - val_loss: 1.2173 - val_categorical_accuracy: 0.5200 - lr: 5.0000e-04\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.0358 - categorical_accuracy: 0.5274\n",
      "Epoch 2: saving model to model_init_2024-02-0310_09_19.822402/model-00002-1.03581-0.52736-1.20644-0.46000.h5\n",
      "67/67 [==============================] - 178s 3s/step - loss: 1.0358 - categorical_accuracy: 0.5274 - val_loss: 1.2064 - val_categorical_accuracy: 0.4600 - lr: 5.0000e-04\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.0570 - categorical_accuracy: 0.4776\n",
      "Epoch 3: saving model to model_init_2024-02-0310_09_19.822402/model-00003-1.05701-0.47761-1.14896-0.51000.h5\n",
      "67/67 [==============================] - 179s 3s/step - loss: 1.0570 - categorical_accuracy: 0.4776 - val_loss: 1.1490 - val_categorical_accuracy: 0.5100 - lr: 5.0000e-04\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.9125 - categorical_accuracy: 0.6219\n",
      "Epoch 4: saving model to model_init_2024-02-0310_09_19.822402/model-00004-0.91250-0.62189-1.18487-0.51000.h5\n",
      "67/67 [==============================] - 186s 3s/step - loss: 0.9125 - categorical_accuracy: 0.6219 - val_loss: 1.1849 - val_categorical_accuracy: 0.5100 - lr: 5.0000e-04\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.7892 - categorical_accuracy: 0.6716\n",
      "Epoch 5: saving model to model_init_2024-02-0310_09_19.822402/model-00005-0.78918-0.67164-1.19712-0.53000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "67/67 [==============================] - 182s 3s/step - loss: 0.7892 - categorical_accuracy: 0.6716 - val_loss: 1.1971 - val_categorical_accuracy: 0.5300 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.8840 - categorical_accuracy: 0.6368\n",
      "Epoch 6: saving model to model_init_2024-02-0310_09_19.822402/model-00006-0.88404-0.63682-1.14087-0.53000.h5\n",
      "67/67 [==============================] - 183s 3s/step - loss: 0.8840 - categorical_accuracy: 0.6368 - val_loss: 1.1409 - val_categorical_accuracy: 0.5300 - lr: 2.5000e-04\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.8385 - categorical_accuracy: 0.6567\n",
      "Epoch 7: saving model to model_init_2024-02-0310_09_19.822402/model-00007-0.83849-0.65672-1.17390-0.50000.h5\n",
      "67/67 [==============================] - 184s 3s/step - loss: 0.8385 - categorical_accuracy: 0.6567 - val_loss: 1.1739 - val_categorical_accuracy: 0.5000 - lr: 2.5000e-04\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.6430 - categorical_accuracy: 0.7264\n",
      "Epoch 8: saving model to model_init_2024-02-0310_09_19.822402/model-00008-0.64298-0.72637-1.27574-0.52000.h5\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "67/67 [==============================] - 183s 3s/step - loss: 0.6430 - categorical_accuracy: 0.7264 - val_loss: 1.2757 - val_categorical_accuracy: 0.5200 - lr: 2.5000e-04\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.6818 - categorical_accuracy: 0.7065\n",
      "Epoch 9: saving model to model_init_2024-02-0310_09_19.822402/model-00009-0.68185-0.70647-1.18685-0.53500.h5\n",
      "67/67 [==============================] - 184s 3s/step - loss: 0.6818 - categorical_accuracy: 0.7065 - val_loss: 1.1868 - val_categorical_accuracy: 0.5350 - lr: 1.2500e-04\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.5796 - categorical_accuracy: 0.7910\n",
      "Epoch 10: saving model to model_init_2024-02-0310_09_19.822402/model-00010-0.57956-0.79104-1.21729-0.58500.h5\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "67/67 [==============================] - 181s 3s/step - loss: 0.5796 - categorical_accuracy: 0.7910 - val_loss: 1.2173 - val_categorical_accuracy: 0.5850 - lr: 1.2500e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff4e0e5a6b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator_ex(train_path, train_doc, 10)\n",
    "val_generator = generator_ex(val_path, val_doc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /Users/smalagi/Desktop/Project_data/train ; batch size = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:19: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/67 [===========================>..] - ETA: 18s - loss: 0.5039 - categorical_accuracy: 0.8094Batch:  67 Index: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:44: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - ETA: 0s - loss: 0.5089 - categorical_accuracy: 0.8069Source path =  /Users/smalagi/Desktop/Project_data/val ; batch size = 10\n",
      "\n",
      "Epoch 1: saving model to model_init_2024-02-0310_09_19.822402/model-00001-0.50887-0.80694-1.23107-0.58000.h5\n",
      "67/67 [==============================] - 434s 6s/step - loss: 0.5089 - categorical_accuracy: 0.8069 - val_loss: 1.2311 - val_categorical_accuracy: 0.5800 - lr: 6.2500e-05\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4961 - categorical_accuracy: 0.8060\n",
      "Epoch 2: saving model to model_init_2024-02-0310_09_19.822402/model-00002-0.49611-0.80597-1.28238-0.53000.h5\n",
      "67/67 [==============================] - 154s 2s/step - loss: 0.4961 - categorical_accuracy: 0.8060 - val_loss: 1.2824 - val_categorical_accuracy: 0.5300 - lr: 6.2500e-05\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.5608 - categorical_accuracy: 0.7761\n",
      "Epoch 3: saving model to model_init_2024-02-0310_09_19.822402/model-00003-0.56078-0.77612-1.34655-0.57000.h5\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "67/67 [==============================] - 145s 2s/step - loss: 0.5608 - categorical_accuracy: 0.7761 - val_loss: 1.3466 - val_categorical_accuracy: 0.5700 - lr: 6.2500e-05\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.5010 - categorical_accuracy: 0.8010\n",
      "Epoch 4: saving model to model_init_2024-02-0310_09_19.822402/model-00004-0.50099-0.80100-1.20310-0.62000.h5\n",
      "67/67 [==============================] - 147s 2s/step - loss: 0.5010 - categorical_accuracy: 0.8010 - val_loss: 1.2031 - val_categorical_accuracy: 0.6200 - lr: 3.1250e-05\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4142 - categorical_accuracy: 0.8557\n",
      "Epoch 5: saving model to model_init_2024-02-0310_09_19.822402/model-00005-0.41424-0.85572-1.20026-0.58000.h5\n",
      "67/67 [==============================] - 145s 2s/step - loss: 0.4142 - categorical_accuracy: 0.8557 - val_loss: 1.2003 - val_categorical_accuracy: 0.5800 - lr: 3.1250e-05\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4644 - categorical_accuracy: 0.8408\n",
      "Epoch 6: saving model to model_init_2024-02-0310_09_19.822402/model-00006-0.46440-0.84080-1.30355-0.55000.h5\n",
      "67/67 [==============================] - 147s 2s/step - loss: 0.4644 - categorical_accuracy: 0.8408 - val_loss: 1.3035 - val_categorical_accuracy: 0.5500 - lr: 3.1250e-05\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.5062 - categorical_accuracy: 0.8159\n",
      "Epoch 7: saving model to model_init_2024-02-0310_09_19.822402/model-00007-0.50621-0.81592-1.31748-0.58000.h5\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "67/67 [==============================] - 146s 2s/step - loss: 0.5062 - categorical_accuracy: 0.8159 - val_loss: 1.3175 - val_categorical_accuracy: 0.5800 - lr: 3.1250e-05\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4649 - categorical_accuracy: 0.8209\n",
      "Epoch 8: saving model to model_init_2024-02-0310_09_19.822402/model-00008-0.46494-0.82090-1.25979-0.61000.h5\n",
      "67/67 [==============================] - 151s 2s/step - loss: 0.4649 - categorical_accuracy: 0.8209 - val_loss: 1.2598 - val_categorical_accuracy: 0.6100 - lr: 1.5625e-05\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4135 - categorical_accuracy: 0.8756\n",
      "Epoch 9: saving model to model_init_2024-02-0310_09_19.822402/model-00009-0.41351-0.87562-1.42145-0.53000.h5\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "67/67 [==============================] - 154s 2s/step - loss: 0.4135 - categorical_accuracy: 0.8756 - val_loss: 1.4214 - val_categorical_accuracy: 0.5300 - lr: 1.5625e-05\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.5593 - categorical_accuracy: 0.7662\n",
      "Epoch 10: saving model to model_init_2024-02-0310_09_19.822402/model-00010-0.55925-0.76617-1.21573-0.61000.h5\n",
      "67/67 [==============================] - 154s 2s/step - loss: 0.5593 - categorical_accuracy: 0.7662 - val_loss: 1.2157 - val_categorical_accuracy: 0.6100 - lr: 7.8125e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff4d07c6fe0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator_ex(train_path, train_doc, 15)\n",
    "val_generator = generator_ex(val_path, val_doc, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /Users/smalagi/Desktop/Project_data/train ; batch size = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:19: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "42/67 [=================>............] - ETA: 3:24 - loss: 0.4480 - categorical_accuracy: 0.8397Batch:  45 Index: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:44: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - ETA: 0s - loss: 0.4465 - categorical_accuracy: 0.8381Source path =  /Users/smalagi/Desktop/Project_data/val ; batch size = 15\n",
      "Batch:  7 Index: 15\n",
      "\n",
      "Epoch 1: saving model to model_init_2024-02-0310_09_19.822402/model-00001-0.44651-0.83813-1.24794-0.57692.h5\n",
      "67/67 [==============================] - 443s 7s/step - loss: 0.4465 - categorical_accuracy: 0.8381 - val_loss: 1.2479 - val_categorical_accuracy: 0.5769 - lr: 7.8125e-06\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4719 - categorical_accuracy: 0.8408\n",
      "Epoch 2: saving model to model_init_2024-02-0310_09_19.822402/model-00002-0.47191-0.84080-1.39794-0.52000.h5\n",
      "67/67 [==============================] - 147s 2s/step - loss: 0.4719 - categorical_accuracy: 0.8408 - val_loss: 1.3979 - val_categorical_accuracy: 0.5200 - lr: 7.8125e-06\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.3771 - categorical_accuracy: 0.8955\n",
      "Epoch 3: saving model to model_init_2024-02-0310_09_19.822402/model-00003-0.37707-0.89552-1.30773-0.59000.h5\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "67/67 [==============================] - 159s 2s/step - loss: 0.3771 - categorical_accuracy: 0.8955 - val_loss: 1.3077 - val_categorical_accuracy: 0.5900 - lr: 7.8125e-06\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4229 - categorical_accuracy: 0.8507\n",
      "Epoch 4: saving model to model_init_2024-02-0310_09_19.822402/model-00004-0.42287-0.85075-1.14508-0.68000.h5\n",
      "67/67 [==============================] - 161s 2s/step - loss: 0.4229 - categorical_accuracy: 0.8507 - val_loss: 1.1451 - val_categorical_accuracy: 0.6800 - lr: 3.9063e-06\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4560 - categorical_accuracy: 0.8408\n",
      "Epoch 5: saving model to model_init_2024-02-0310_09_19.822402/model-00005-0.45597-0.84080-1.42551-0.54000.h5\n",
      "67/67 [==============================] - 157s 2s/step - loss: 0.4560 - categorical_accuracy: 0.8408 - val_loss: 1.4255 - val_categorical_accuracy: 0.5400 - lr: 3.9063e-06\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.3812 - categorical_accuracy: 0.8706\n",
      "Epoch 6: saving model to model_init_2024-02-0310_09_19.822402/model-00006-0.38119-0.87065-1.30472-0.59000.h5\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "67/67 [==============================] - 161s 2s/step - loss: 0.3812 - categorical_accuracy: 0.8706 - val_loss: 1.3047 - val_categorical_accuracy: 0.5900 - lr: 3.9063e-06\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4929 - categorical_accuracy: 0.8010\n",
      "Epoch 7: saving model to model_init_2024-02-0310_09_19.822402/model-00007-0.49290-0.80100-1.23943-0.61000.h5\n",
      "67/67 [==============================] - 160s 2s/step - loss: 0.4929 - categorical_accuracy: 0.8010 - val_loss: 1.2394 - val_categorical_accuracy: 0.6100 - lr: 1.9531e-06\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4668 - categorical_accuracy: 0.8458\n",
      "Epoch 8: saving model to model_init_2024-02-0310_09_19.822402/model-00008-0.46675-0.84577-1.30329-0.59000.h5\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "67/67 [==============================] - 161s 2s/step - loss: 0.4668 - categorical_accuracy: 0.8458 - val_loss: 1.3033 - val_categorical_accuracy: 0.5900 - lr: 1.9531e-06\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4461 - categorical_accuracy: 0.8408\n",
      "Epoch 9: saving model to model_init_2024-02-0310_09_19.822402/model-00009-0.44612-0.84080-1.22243-0.61000.h5\n",
      "67/67 [==============================] - 164s 2s/step - loss: 0.4461 - categorical_accuracy: 0.8408 - val_loss: 1.2224 - val_categorical_accuracy: 0.6100 - lr: 9.7656e-07\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4570 - categorical_accuracy: 0.8358\n",
      "Epoch 10: saving model to model_init_2024-02-0310_09_19.822402/model-00010-0.45701-0.83582-1.26968-0.61000.h5\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "67/67 [==============================] - 167s 2s/step - loss: 0.4570 - categorical_accuracy: 0.8358 - val_loss: 1.2697 - val_categorical_accuracy: 0.6100 - lr: 9.7656e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff4826a05e0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_8 (Conv3D)           (None, 30, 60, 60, 32)    2624      \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 30, 60, 60, 32)    0         \n",
      "                                                                 \n",
      " conv3d_9 (Conv3D)           (None, 30, 60, 60, 32)    27680     \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 30, 60, 60, 32)    0         \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPoolin  (None, 10, 20, 20, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 10, 20, 20, 32)    0         \n",
      "                                                                 \n",
      " conv3d_10 (Conv3D)          (None, 10, 20, 20, 64)    55360     \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 10, 20, 20, 64)    0         \n",
      "                                                                 \n",
      " conv3d_11 (Conv3D)          (None, 10, 20, 20, 64)    110656    \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 10, 20, 20, 64)    0         \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPoolin  (None, 4, 7, 7, 64)       0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 4, 7, 7, 64)       0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               6423040   \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6621925 (25.26 MB)\n",
      "Trainable params: 6621925 (25.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_b.compile(optimizer=tf.keras.optimizers.Adadelta(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator_ex(train_path, train_doc, 16)\n",
    "val_generator = generator_ex(val_path, val_doc, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/3161962229.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_b.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:19: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /Users/smalagi/Desktop/Project_data/train ; batch size = 16\n",
      "Epoch 1/10\n",
      "39/67 [================>.............] - ETA: 4:05 - loss: 0.4475 - categorical_accuracy: 0.8349Batch:  42 Index: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:44: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - ETA: 0s - loss: 0.4401 - categorical_accuracy: 0.8413Source path =  /Users/smalagi/Desktop/Project_data/val ; batch size = 16\n",
      "Batch:  7 Index: 16\n",
      "\n",
      "Epoch 1: saving model to model_init_2024-02-0310_09_19.822402/model-00001-0.44005-0.84129-1.32737-0.56250.h5\n",
      "67/67 [==============================] - 512s 8s/step - loss: 0.4401 - categorical_accuracy: 0.8413 - val_loss: 1.3274 - val_categorical_accuracy: 0.5625 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4106 - categorical_accuracy: 0.8401\n",
      "Epoch 2: saving model to model_init_2024-02-0310_09_19.822402/model-00002-0.41064-0.84009-1.21635-0.57500.h5\n",
      "67/67 [==============================] - 279s 4s/step - loss: 0.4106 - categorical_accuracy: 0.8401 - val_loss: 1.2163 - val_categorical_accuracy: 0.5750 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "Batch:  95 Index: 7\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4253 - categorical_accuracy: 0.8407\n",
      "Epoch 3: saving model to model_init_2024-02-0310_09_19.822402/model-00003-0.42527-0.84071-1.34340-0.62500.h5\n",
      "67/67 [==============================] - 218s 3s/step - loss: 0.4253 - categorical_accuracy: 0.8407 - val_loss: 1.3434 - val_categorical_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "66/67 [============================>.] - ETA: 2s - loss: 0.4125 - categorical_accuracy: 0.8576Batch:  133 Index: 5\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4103 - categorical_accuracy: 0.8597\n",
      "Epoch 4: saving model to model_init_2024-02-0310_09_19.822402/model-00004-0.41033-0.85970-1.28307-0.55000.h5\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "67/67 [==============================] - 205s 3s/step - loss: 0.4103 - categorical_accuracy: 0.8597 - val_loss: 1.2831 - val_categorical_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.3781 - categorical_accuracy: 0.8522\n",
      "Epoch 5: saving model to model_init_2024-02-0310_09_19.822402/model-00005-0.37813-0.85222-1.60107-0.55000.h5\n",
      "67/67 [==============================] - 129s 2s/step - loss: 0.3781 - categorical_accuracy: 0.8522 - val_loss: 1.6011 - val_categorical_accuracy: 0.5500 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4509 - categorical_accuracy: 0.8259\n",
      "Epoch 6: saving model to model_init_2024-02-0310_09_19.822402/model-00006-0.45087-0.82587-1.11824-0.65000.h5\n",
      "67/67 [==============================] - 126s 2s/step - loss: 0.4509 - categorical_accuracy: 0.8259 - val_loss: 1.1182 - val_categorical_accuracy: 0.6500 - lr: 5.0000e-04\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4741 - categorical_accuracy: 0.8259\n",
      "Epoch 7: saving model to model_init_2024-02-0310_09_19.822402/model-00007-0.47413-0.82587-1.36778-0.62500.h5\n",
      "67/67 [==============================] - 135s 2s/step - loss: 0.4741 - categorical_accuracy: 0.8259 - val_loss: 1.3678 - val_categorical_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4300 - categorical_accuracy: 0.8408\n",
      "Epoch 8: saving model to model_init_2024-02-0310_09_19.822402/model-00008-0.43002-0.84080-1.48543-0.60000.h5\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "67/67 [==============================] - 131s 2s/step - loss: 0.4300 - categorical_accuracy: 0.8408 - val_loss: 1.4854 - val_categorical_accuracy: 0.6000 - lr: 5.0000e-04\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4228 - categorical_accuracy: 0.8507\n",
      "Epoch 9: saving model to model_init_2024-02-0310_09_19.822402/model-00009-0.42277-0.85075-1.17327-0.57500.h5\n",
      "67/67 [==============================] - 128s 2s/step - loss: 0.4228 - categorical_accuracy: 0.8507 - val_loss: 1.1733 - val_categorical_accuracy: 0.5750 - lr: 2.5000e-04\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4647 - categorical_accuracy: 0.8109\n",
      "Epoch 10: saving model to model_init_2024-02-0310_09_19.822402/model-00010-0.46473-0.81095-1.31119-0.60000.h5\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "67/67 [==============================] - 126s 2s/step - loss: 0.4647 - categorical_accuracy: 0.8109 - val_loss: 1.3112 - val_categorical_accuracy: 0.6000 - lr: 2.5000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7febd064c1c0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# epochs = 18\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 18 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_8 (Conv3D)           (None, 30, 60, 60, 32)    2624      \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 30, 60, 60, 32)    0         \n",
      "                                                                 \n",
      " conv3d_9 (Conv3D)           (None, 30, 60, 60, 32)    27680     \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 30, 60, 60, 32)    0         \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPoolin  (None, 10, 20, 20, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 10, 20, 20, 32)    0         \n",
      "                                                                 \n",
      " conv3d_10 (Conv3D)          (None, 10, 20, 20, 64)    55360     \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 10, 20, 20, 64)    0         \n",
      "                                                                 \n",
      " conv3d_11 (Conv3D)          (None, 10, 20, 20, 64)    110656    \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 10, 20, 20, 64)    0         \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPoolin  (None, 4, 7, 7, 64)       0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 4, 7, 7, 64)       0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               6423040   \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6621925 (25.26 MB)\n",
      "Trainable params: 6621925 (25.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "model_b.compile(optimizer=tf.keras.optimizers.Adadelta(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator_ex(train_path, train_doc, 18)\n",
    "val_generator = generator_ex(val_path, val_doc, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/3161962229.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_b.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:19: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /Users/smalagi/Desktop/Project_data/train ; batch size = 18\n",
      "Epoch 1/18\n",
      "34/67 [==============>...............] - ETA: 5:33 - loss: 0.4047 - categorical_accuracy: 0.8382Batch:  37 Index: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:44: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - ETA: 0s - loss: 0.4029 - categorical_accuracy: 0.8446Source path =  /Users/smalagi/Desktop/Project_data/val ; batch size = 18\n",
      "Batch:  6 Index: 18\n",
      "\n",
      "Epoch 1: saving model to model_init_2024-02-0310_09_19.822402/model-00001-0.40287-0.84456-1.33645-0.56429.h5\n",
      "67/67 [==============================] - 673s 10s/step - loss: 0.4029 - categorical_accuracy: 0.8446 - val_loss: 1.3364 - val_categorical_accuracy: 0.5643 - lr: 0.0010\n",
      "Epoch 2/18\n",
      "12/67 [====>.........................] - ETA: 7:41 - loss: 0.5173 - categorical_accuracy: 0.8333Batch:  45 Index: 15\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4839 - categorical_accuracy: 0.8184\n",
      "Epoch 2: saving model to model_init_2024-02-0310_09_19.822402/model-00002-0.48391-0.81843-1.36662-0.59000.h5\n",
      "67/67 [==============================] - 241s 4s/step - loss: 0.4839 - categorical_accuracy: 0.8184 - val_loss: 1.3666 - val_categorical_accuracy: 0.5900 - lr: 0.0010\n",
      "Epoch 3/18\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4311 - categorical_accuracy: 0.8308\n",
      "Epoch 3: saving model to model_init_2024-02-0310_09_19.822402/model-00003-0.43106-0.83085-1.14494-0.64000.h5\n",
      "67/67 [==============================] - 147s 2s/step - loss: 0.4311 - categorical_accuracy: 0.8308 - val_loss: 1.1449 - val_categorical_accuracy: 0.6400 - lr: 0.0010\n",
      "Epoch 4/18\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4158 - categorical_accuracy: 0.8408\n",
      "Epoch 4: saving model to model_init_2024-02-0310_09_19.822402/model-00004-0.41579-0.84080-1.30959-0.58000.h5\n",
      "67/67 [==============================] - 148s 2s/step - loss: 0.4158 - categorical_accuracy: 0.8408 - val_loss: 1.3096 - val_categorical_accuracy: 0.5800 - lr: 0.0010\n",
      "Epoch 5/18\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.3965 - categorical_accuracy: 0.8458\n",
      "Epoch 5: saving model to model_init_2024-02-0310_09_19.822402/model-00005-0.39655-0.84577-1.39491-0.56000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "67/67 [==============================] - 147s 2s/step - loss: 0.3965 - categorical_accuracy: 0.8458 - val_loss: 1.3949 - val_categorical_accuracy: 0.5600 - lr: 0.0010\n",
      "Epoch 6/18\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.3927 - categorical_accuracy: 0.8408\n",
      "Epoch 6: saving model to model_init_2024-02-0310_09_19.822402/model-00006-0.39273-0.84080-1.28749-0.59000.h5\n",
      "67/67 [==============================] - 146s 2s/step - loss: 0.3927 - categorical_accuracy: 0.8408 - val_loss: 1.2875 - val_categorical_accuracy: 0.5900 - lr: 5.0000e-04\n",
      "Epoch 7/18\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4496 - categorical_accuracy: 0.8308\n",
      "Epoch 7: saving model to model_init_2024-02-0310_09_19.822402/model-00007-0.44959-0.83085-1.30920-0.59000.h5\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "67/67 [==============================] - 148s 2s/step - loss: 0.4496 - categorical_accuracy: 0.8308 - val_loss: 1.3092 - val_categorical_accuracy: 0.5900 - lr: 5.0000e-04\n",
      "Epoch 8/18\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4237 - categorical_accuracy: 0.8458\n",
      "Epoch 8: saving model to model_init_2024-02-0310_09_19.822402/model-00008-0.42374-0.84577-1.27957-0.58000.h5\n",
      "67/67 [==============================] - 143s 2s/step - loss: 0.4237 - categorical_accuracy: 0.8458 - val_loss: 1.2796 - val_categorical_accuracy: 0.5800 - lr: 2.5000e-04\n",
      "Epoch 9/18\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4867 - categorical_accuracy: 0.8308\n",
      "Epoch 9: saving model to model_init_2024-02-0310_09_19.822402/model-00009-0.48667-0.83085-1.24406-0.59000.h5\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "67/67 [==============================] - 147s 2s/step - loss: 0.4867 - categorical_accuracy: 0.8308 - val_loss: 1.2441 - val_categorical_accuracy: 0.5900 - lr: 2.5000e-04\n",
      "Epoch 10/18\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4169 - categorical_accuracy: 0.8607\n",
      "Epoch 10: saving model to model_init_2024-02-0310_09_19.822402/model-00010-0.41693-0.86070-1.37091-0.58000.h5\n",
      "67/67 [==============================] - 149s 2s/step - loss: 0.4169 - categorical_accuracy: 0.8607 - val_loss: 1.3709 - val_categorical_accuracy: 0.5800 - lr: 1.2500e-04\n",
      "Epoch 11/18\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.3976 - categorical_accuracy: 0.8557\n",
      "Epoch 11: saving model to model_init_2024-02-0310_09_19.822402/model-00011-0.39757-0.85572-1.29576-0.58000.h5\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "67/67 [==============================] - 146s 2s/step - loss: 0.3976 - categorical_accuracy: 0.8557 - val_loss: 1.2958 - val_categorical_accuracy: 0.5800 - lr: 1.2500e-04\n",
      "Epoch 12/18\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4756 - categorical_accuracy: 0.8109\n",
      "Epoch 12: saving model to model_init_2024-02-0310_09_19.822402/model-00012-0.47558-0.81095-1.22230-0.62000.h5\n",
      "67/67 [==============================] - 147s 2s/step - loss: 0.4756 - categorical_accuracy: 0.8109 - val_loss: 1.2223 - val_categorical_accuracy: 0.6200 - lr: 6.2500e-05\n",
      "Epoch 13/18\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.3755 - categorical_accuracy: 0.8806\n",
      "Epoch 13: saving model to model_init_2024-02-0310_09_19.822402/model-00013-0.37545-0.88060-1.47922-0.56000.h5\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "67/67 [==============================] - 145s 2s/step - loss: 0.3755 - categorical_accuracy: 0.8806 - val_loss: 1.4792 - val_categorical_accuracy: 0.5600 - lr: 6.2500e-05\n",
      "Epoch 14/18\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4240 - categorical_accuracy: 0.8607\n",
      "Epoch 14: saving model to model_init_2024-02-0310_09_19.822402/model-00014-0.42403-0.86070-1.14840-0.62000.h5\n",
      "67/67 [==============================] - 153s 2s/step - loss: 0.4240 - categorical_accuracy: 0.8607 - val_loss: 1.1484 - val_categorical_accuracy: 0.6200 - lr: 3.1250e-05\n",
      "Epoch 15/18\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4147 - categorical_accuracy: 0.8607\n",
      "Epoch 15: saving model to model_init_2024-02-0310_09_19.822402/model-00015-0.41468-0.86070-1.40242-0.56000.h5\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "67/67 [==============================] - 157s 2s/step - loss: 0.4147 - categorical_accuracy: 0.8607 - val_loss: 1.4024 - val_categorical_accuracy: 0.5600 - lr: 3.1250e-05\n",
      "Epoch 16/18\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4007 - categorical_accuracy: 0.8507\n",
      "Epoch 16: saving model to model_init_2024-02-0310_09_19.822402/model-00016-0.40072-0.85075-1.34103-0.57000.h5\n",
      "67/67 [==============================] - 148s 2s/step - loss: 0.4007 - categorical_accuracy: 0.8507 - val_loss: 1.3410 - val_categorical_accuracy: 0.5700 - lr: 1.5625e-05\n",
      "Epoch 17/18\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4456 - categorical_accuracy: 0.8209\n",
      "Epoch 17: saving model to model_init_2024-02-0310_09_19.822402/model-00017-0.44555-0.82090-1.30933-0.59000.h5\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "67/67 [==============================] - 150s 2s/step - loss: 0.4456 - categorical_accuracy: 0.8209 - val_loss: 1.3093 - val_categorical_accuracy: 0.5900 - lr: 1.5625e-05\n",
      "Epoch 18/18\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4838 - categorical_accuracy: 0.8358\n",
      "Epoch 18: saving model to model_init_2024-02-0310_09_19.822402/model-00018-0.48382-0.83582-1.33197-0.59000.h5\n",
      "67/67 [==============================] - 148s 2s/step - loss: 0.4838 - categorical_accuracy: 0.8358 - val_loss: 1.3320 - val_categorical_accuracy: 0.5900 - lr: 7.8125e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff48a492a10>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_12 (Conv3D)          (None, 30, 60, 60, 32)    2624      \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 30, 60, 60, 32)    0         \n",
      "                                                                 \n",
      " conv3d_13 (Conv3D)          (None, 30, 60, 60, 32)    27680     \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 30, 60, 60, 32)    0         \n",
      "                                                                 \n",
      " max_pooling3d_8 (MaxPoolin  (None, 10, 20, 20, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 10, 20, 20, 32)    0         \n",
      "                                                                 \n",
      " conv3d_14 (Conv3D)          (None, 10, 20, 20, 64)    55360     \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 10, 20, 20, 64)    0         \n",
      "                                                                 \n",
      " conv3d_15 (Conv3D)          (None, 10, 20, 20, 64)    110656    \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 10, 20, 20, 64)    0         \n",
      "                                                                 \n",
      " max_pooling3d_9 (MaxPoolin  (None, 4, 7, 7, 64)       0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 4, 7, 7, 64)       0         \n",
      "                                                                 \n",
      " conv3d_16 (Conv3D)          (None, 4, 7, 7, 64)       110656    \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 4, 7, 7, 64)       0         \n",
      "                                                                 \n",
      " conv3d_17 (Conv3D)          (None, 4, 7, 7, 64)       110656    \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 4, 7, 7, 64)       0         \n",
      "                                                                 \n",
      " max_pooling3d_10 (MaxPooli  (None, 2, 3, 3, 64)       0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 2, 3, 3, 64)       0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               590336    \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1012581 (3.86 MB)\n",
      "Trainable params: 1011557 (3.86 MB)\n",
      "Non-trainable params: 1024 (4.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_c = Sequential()\n",
    "model_c.add(Conv3D(32, kernel_size=(3, 3, 3), input_shape=(x,y,z,channel), padding=\"same\"))\n",
    "model_c.add(Activation('relu'))\n",
    "model_c.add(Conv3D(32, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model_c.add(Activation('relu'))\n",
    "model_c.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n",
    "model_c.add(Dropout(0.25))\n",
    "\n",
    "model_c.add(Conv3D(64, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model_c.add(Activation('relu'))\n",
    "model_c.add(Conv3D(64, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model_c.add(Activation('relu'))\n",
    "model_c.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n",
    "model_c.add(Dropout(0.25))\n",
    "\n",
    "model_c.add(Conv3D(64, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model_c.add(Activation('relu'))\n",
    "model_c.add(Conv3D(64, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model_c.add(Activation('relu'))\n",
    "model_c.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n",
    "model_c.add(Dropout(0.25))\n",
    "\n",
    "model_c.add(Flatten())\n",
    "model_c.add(Dense(512, activation='relu'))\n",
    "model_c.add(BatchNormalization())\n",
    "model_c.add(Dropout(0.5))\n",
    "model_c.add(Dense(classes, activation='softmax'))\n",
    "model_c.compile(optimizer=tf.keras.optimizers.Adadelta(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_c.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing back batch size, images per frame, height and width of image\n",
    "batch_size = 10\n",
    "x = 30 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120# image height\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator_ex(train_path, train_doc, batch_size)\n",
    "val_generator = generator_ex(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_18 (Conv3D)          (None, 30, 120, 120, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 30, 120, 120, 8)   32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " max_pooling3d_11 (MaxPooli  (None, 15, 60, 60, 8)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_19 (Conv3D)          (None, 15, 60, 60, 16)    3472      \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 15, 60, 60, 16)    64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 15, 60, 60, 16)    0         \n",
      "                                                                 \n",
      " max_pooling3d_12 (MaxPooli  (None, 7, 30, 30, 16)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_20 (Conv3D)          (None, 7, 30, 30, 32)     4640      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 7, 30, 30, 32)     128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 7, 30, 30, 32)     0         \n",
      "                                                                 \n",
      " max_pooling3d_13 (MaxPooli  (None, 3, 15, 15, 32)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_21 (Conv3D)          (None, 3, 15, 15, 64)     18496     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 3, 15, 15, 64)     256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_21 (Activation)  (None, 3, 15, 15, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d_14 (MaxPooli  (None, 1, 7, 7, 64)       0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1000)              3137000   \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3667749 (13.99 MB)\n",
      "Trainable params: 3667509 (13.99 MB)\n",
      "Non-trainable params: 240 (960.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape=(x,y,z,channel)\n",
    "\n",
    "nb_filters = [8,16,32,64]\n",
    "nb_dense = [1000, 500, 5]\n",
    "# Define model\n",
    "model_d = Sequential()\n",
    "\n",
    "model_d.add(Conv3D(nb_filters[0], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "model_d.add(BatchNormalization())\n",
    "model_d.add(Activation('relu'))\n",
    "\n",
    "model_d.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_d.add(Conv3D(nb_filters[1], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model_d.add(BatchNormalization())\n",
    "model_d.add(Activation('relu'))\n",
    "\n",
    "model_d.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_d.add(Conv3D(nb_filters[2], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model_d.add(BatchNormalization())\n",
    "model_d.add(Activation('relu'))\n",
    "\n",
    "model_d.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_d.add(Conv3D(nb_filters[3], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model_d.add(BatchNormalization())\n",
    "model_d.add(Activation('relu'))\n",
    "\n",
    "model_d.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model_d.add(Flatten())\n",
    "\n",
    "model_d.add(Dense(nb_dense[0], activation='relu'))\n",
    "model_d.add(Dropout(0.5))\n",
    "\n",
    "model_d.add(Dense(nb_dense[1], activation='relu'))\n",
    "model_d.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model_d.add(Dense(nb_dense[2], activation='softmax'))\n",
    "model_d.compile(optimizer=tf.keras.optimizers.Adadelta(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/3797531445.py:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_d.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:19: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /Users/smalagi/Desktop/Project_data/train ; batch size = 10\n",
      "Epoch 1/10\n",
      "64/67 [===========================>..] - ETA: 13s - loss: 3.2176 - categorical_accuracy: 0.2125Batch:  67 Index: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:44: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - ETA: 0s - loss: 3.2095 - categorical_accuracy: 0.2081Source path =  /Users/smalagi/Desktop/Project_data/val ; batch size = 10\n",
      "\n",
      "Epoch 1: saving model to model_init_2024-02-0310_09_19.822402/model-00001-3.20947-0.20814-1.61689-0.25000.h5\n",
      "67/67 [==============================] - 337s 5s/step - loss: 3.2095 - categorical_accuracy: 0.2081 - val_loss: 1.6169 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.8139 - categorical_accuracy: 0.2786\n",
      "Epoch 2: saving model to model_init_2024-02-0310_09_19.822402/model-00002-2.81391-0.27861-1.62266-0.25000.h5\n",
      "67/67 [==============================] - 139s 2s/step - loss: 2.8139 - categorical_accuracy: 0.2786 - val_loss: 1.6227 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.3033 - categorical_accuracy: 0.1791\n",
      "Epoch 3: saving model to model_init_2024-02-0310_09_19.822402/model-00003-3.30330-0.17910-1.61972-0.22000.h5\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "67/67 [==============================] - 138s 2s/step - loss: 3.3033 - categorical_accuracy: 0.1791 - val_loss: 1.6197 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.8311 - categorical_accuracy: 0.2488\n",
      "Epoch 4: saving model to model_init_2024-02-0310_09_19.822402/model-00004-2.83105-0.24876-1.72174-0.14000.h5\n",
      "67/67 [==============================] - 141s 2s/step - loss: 2.8311 - categorical_accuracy: 0.2488 - val_loss: 1.7217 - val_categorical_accuracy: 0.1400 - lr: 5.0000e-04\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.0069 - categorical_accuracy: 0.2139\n",
      "Epoch 5: saving model to model_init_2024-02-0310_09_19.822402/model-00005-3.00686-0.21393-1.64258-0.21000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "67/67 [==============================] - 136s 2s/step - loss: 3.0069 - categorical_accuracy: 0.2139 - val_loss: 1.6426 - val_categorical_accuracy: 0.2100 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.9458 - categorical_accuracy: 0.1791\n",
      "Epoch 6: saving model to model_init_2024-02-0310_09_19.822402/model-00006-2.94581-0.17910-1.77823-0.16000.h5\n",
      "67/67 [==============================] - 142s 2s/step - loss: 2.9458 - categorical_accuracy: 0.1791 - val_loss: 1.7782 - val_categorical_accuracy: 0.1600 - lr: 2.5000e-04\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.8199 - categorical_accuracy: 0.2338\n",
      "Epoch 7: saving model to model_init_2024-02-0310_09_19.822402/model-00007-2.81990-0.23383-1.73482-0.22000.h5\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "67/67 [==============================] - 138s 2s/step - loss: 2.8199 - categorical_accuracy: 0.2338 - val_loss: 1.7348 - val_categorical_accuracy: 0.2200 - lr: 2.5000e-04\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.6888 - categorical_accuracy: 0.2388\n",
      "Epoch 8: saving model to model_init_2024-02-0310_09_19.822402/model-00008-2.68879-0.23881-1.67331-0.26000.h5\n",
      "67/67 [==============================] - 138s 2s/step - loss: 2.6888 - categorical_accuracy: 0.2388 - val_loss: 1.6733 - val_categorical_accuracy: 0.2600 - lr: 1.2500e-04\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.1172 - categorical_accuracy: 0.2040\n",
      "Epoch 9: saving model to model_init_2024-02-0310_09_19.822402/model-00009-3.11725-0.20398-1.77389-0.19000.h5\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "67/67 [==============================] - 136s 2s/step - loss: 3.1172 - categorical_accuracy: 0.2040 - val_loss: 1.7739 - val_categorical_accuracy: 0.1900 - lr: 1.2500e-04\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.8573 - categorical_accuracy: 0.1692\n",
      "Epoch 10: saving model to model_init_2024-02-0310_09_19.822402/model-00010-2.85729-0.16915-1.75268-0.27000.h5\n",
      "67/67 [==============================] - 139s 2s/step - loss: 2.8573 - categorical_accuracy: 0.1692 - val_loss: 1.7527 - val_categorical_accuracy: 0.2700 - lr: 6.2500e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff4f1e74670>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = generator_ex(train_path, train_doc, batch_size)\n",
    "val_generator = generator_ex(val_path, val_doc, batch_size)\n",
    "num_epochs = 10\n",
    "model_d.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 30 # number of frames\n",
    "y = 64 # image width\n",
    "z = 64 # image height \n",
    "\n",
    "classes = 5\n",
    "channel = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_22 (Conv3D)          (None, 30, 64, 64, 32)    896       \n",
      "                                                                 \n",
      " activation_22 (Activation)  (None, 30, 64, 64, 32)    0         \n",
      "                                                                 \n",
      " conv3d_23 (Conv3D)          (None, 30, 64, 64, 32)    27680     \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 30, 64, 64, 32)    0         \n",
      "                                                                 \n",
      " max_pooling3d_15 (MaxPooli  (None, 10, 22, 22, 32)    0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 10, 22, 22, 32)    0         \n",
      "                                                                 \n",
      " conv3d_24 (Conv3D)          (None, 10, 22, 22, 64)    55360     \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 10, 22, 22, 64)    0         \n",
      "                                                                 \n",
      " conv3d_25 (Conv3D)          (None, 10, 22, 22, 64)    110656    \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 10, 22, 22, 64)    0         \n",
      "                                                                 \n",
      " max_pooling3d_16 (MaxPooli  (None, 4, 8, 8, 64)       0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 4, 8, 8, 64)       0         \n",
      "                                                                 \n",
      " conv3d_26 (Conv3D)          (None, 4, 8, 8, 64)       110656    \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 4, 8, 8, 64)       0         \n",
      "                                                                 \n",
      " conv3d_27 (Conv3D)          (None, 4, 8, 8, 64)       110656    \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 4, 8, 8, 64)       0         \n",
      "                                                                 \n",
      " max_pooling3d_17 (MaxPooli  (None, 2, 3, 3, 64)       0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 2, 3, 3, 64)       0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 512)               590336    \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1010853 (3.86 MB)\n",
      "Trainable params: 1009829 (3.85 MB)\n",
      "Non-trainable params: 1024 (4.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_e = Sequential()\n",
    "model_e.add(Conv3D(32, kernel_size=(3, 3, 3), input_shape=(x,y,z,channel), padding=\"same\"))\n",
    "model_e.add(Activation('relu'))\n",
    "model_e.add(Conv3D(32, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model_e.add(Activation('relu'))\n",
    "model_e.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n",
    "model_e.add(Dropout(0.25))\n",
    "\n",
    "model_e.add(Conv3D(64, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model_e.add(Activation('relu'))\n",
    "model_e.add(Conv3D(64, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model_e.add(Activation('relu'))\n",
    "model_e.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n",
    "model_e.add(Dropout(0.25))\n",
    "\n",
    "model_e.add(Conv3D(64, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model_e.add(Activation('relu'))\n",
    "model_e.add(Conv3D(64, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model_e.add(Activation('relu'))\n",
    "model_e.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n",
    "model_e.add(Dropout(0.25))\n",
    "\n",
    "model_e.add(Flatten())\n",
    "model_e.add(Dense(512, activation='relu'))\n",
    "model_e.add(BatchNormalization())\n",
    "model_e.add(Dropout(0.5))\n",
    "model_e.add(Dense(classes, activation='softmax'))\n",
    "model_e.compile(optimizer=tf.keras.optimizers.Adadelta(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_e.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/1728685948.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_e.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:19: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /Users/smalagi/Desktop/Project_data/train ; batch size = 5\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.0358 - categorical_accuracy: 0.1940Source path =  /Users/smalagi/Desktop/Project_data/val ; batch size = 5\n",
      "\n",
      "Epoch 1: saving model to model_init_2024-02-0310_09_19.822402/model-00001-2.03575-0.19403-1.60972-0.20000.h5\n",
      "67/67 [==============================] - 235s 3s/step - loss: 2.0358 - categorical_accuracy: 0.1940 - val_loss: 1.6097 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "63/67 [===========================>..] - ETA: 12s - loss: 2.0301 - categorical_accuracy: 0.1810Batch:  133 Index: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:44: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - ETA: 0s - loss: 2.0452 - categorical_accuracy: 0.1813\n",
      "Epoch 2: saving model to model_init_2024-02-0310_09_19.822402/model-00002-2.04515-0.18127-1.61515-0.16000.h5\n",
      "67/67 [==============================] - 230s 3s/step - loss: 2.0452 - categorical_accuracy: 0.1813 - val_loss: 1.6152 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.0531 - categorical_accuracy: 0.1741\n",
      "Epoch 3: saving model to model_init_2024-02-0310_09_19.822402/model-00003-2.05314-0.17413-1.61569-0.18000.h5\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "67/67 [==============================] - 147s 2s/step - loss: 2.0531 - categorical_accuracy: 0.1741 - val_loss: 1.6157 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.9087 - categorical_accuracy: 0.2189\n",
      "Epoch 4: saving model to model_init_2024-02-0310_09_19.822402/model-00004-1.90871-0.21891-1.61949-0.16000.h5\n",
      "67/67 [==============================] - 147s 2s/step - loss: 1.9087 - categorical_accuracy: 0.2189 - val_loss: 1.6195 - val_categorical_accuracy: 0.1600 - lr: 5.0000e-04\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.9725 - categorical_accuracy: 0.1692\n",
      "Epoch 5: saving model to model_init_2024-02-0310_09_19.822402/model-00005-1.97253-0.16915-1.63002-0.16000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "67/67 [==============================] - 147s 2s/step - loss: 1.9725 - categorical_accuracy: 0.1692 - val_loss: 1.6300 - val_categorical_accuracy: 0.1600 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.8277 - categorical_accuracy: 0.2289\n",
      "Epoch 6: saving model to model_init_2024-02-0310_09_19.822402/model-00006-1.82773-0.22886-1.65306-0.12000.h5\n",
      "67/67 [==============================] - 148s 2s/step - loss: 1.8277 - categorical_accuracy: 0.2289 - val_loss: 1.6531 - val_categorical_accuracy: 0.1200 - lr: 2.5000e-04\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.7370 - categorical_accuracy: 0.2786\n",
      "Epoch 7: saving model to model_init_2024-02-0310_09_19.822402/model-00007-1.73703-0.27861-1.65675-0.20000.h5\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "67/67 [==============================] - 149s 2s/step - loss: 1.7370 - categorical_accuracy: 0.2786 - val_loss: 1.6567 - val_categorical_accuracy: 0.2000 - lr: 2.5000e-04\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.9278 - categorical_accuracy: 0.1741\n",
      "Epoch 8: saving model to model_init_2024-02-0310_09_19.822402/model-00008-1.92780-0.17413-1.74342-0.16000.h5\n",
      "67/67 [==============================] - 145s 2s/step - loss: 1.9278 - categorical_accuracy: 0.1741 - val_loss: 1.7434 - val_categorical_accuracy: 0.1600 - lr: 1.2500e-04\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.9501 - categorical_accuracy: 0.1741\n",
      "Epoch 9: saving model to model_init_2024-02-0310_09_19.822402/model-00009-1.95014-0.17413-1.71675-0.20000.h5\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "67/67 [==============================] - 148s 2s/step - loss: 1.9501 - categorical_accuracy: 0.1741 - val_loss: 1.7168 - val_categorical_accuracy: 0.2000 - lr: 1.2500e-04\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.0060 - categorical_accuracy: 0.1692\n",
      "Epoch 10: saving model to model_init_2024-02-0310_09_19.822402/model-00010-2.00602-0.16915-1.82955-0.16000.h5\n",
      "67/67 [==============================] - 148s 2s/step - loss: 2.0060 - categorical_accuracy: 0.1692 - val_loss: 1.8296 - val_categorical_accuracy: 0.1600 - lr: 6.2500e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff4826e63e0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "train_generator = generator_ex(train_path, train_doc, batch_size)\n",
    "val_generator = generator_ex(val_path, val_doc, batch_size)\n",
    "num_epochs = 10\n",
    "model_e.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_28 (Conv3D)          (None, 30, 120, 120, 8)   224       \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 30, 120, 120, 8)   32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " max_pooling3d_18 (MaxPooli  (None, 15, 60, 60, 8)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_29 (Conv3D)          (None, 15, 60, 60, 16)    3472      \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 15, 60, 60, 16)    64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 15, 60, 60, 16)    0         \n",
      "                                                                 \n",
      " max_pooling3d_19 (MaxPooli  (None, 7, 30, 30, 16)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_30 (Conv3D)          (None, 7, 30, 30, 32)     4640      \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 7, 30, 30, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 7, 30, 30, 32)     0         \n",
      "                                                                 \n",
      " max_pooling3d_20 (MaxPooli  (None, 3, 15, 15, 32)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_31 (Conv3D)          (None, 3, 15, 15, 64)     18496     \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 3, 15, 15, 64)     256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_31 (Activation)  (None, 3, 15, 15, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d_21 (MaxPooli  (None, 1, 7, 7, 64)       0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1000)              3137000   \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3667317 (13.99 MB)\n",
      "Trainable params: 3667077 (13.99 MB)\n",
      "Non-trainable params: 240 (960.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = 30 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height \n",
    "\n",
    "classes = 5\n",
    "channel = 1\n",
    "nb_dense = [1000, 500, 5]\n",
    "\n",
    "input_shape=(x,y,z,channel)\n",
    "\n",
    "# Define model\n",
    "model_f = Sequential()\n",
    "\n",
    "model_f.add(Conv3D(nb_filters[0], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "model_f.add(BatchNormalization())\n",
    "model_f.add(Activation('relu'))\n",
    "\n",
    "model_f.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_f.add(Conv3D(nb_filters[1], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model_f.add(BatchNormalization())\n",
    "model_f.add(Activation('relu'))\n",
    "\n",
    "model_f.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_f.add(Conv3D(nb_filters[2], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model_f.add(BatchNormalization())\n",
    "model_f.add(Activation('relu'))\n",
    "\n",
    "model_f.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_f.add(Conv3D(nb_filters[3], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model_f.add(BatchNormalization())\n",
    "model_f.add(Activation('relu'))\n",
    "\n",
    "model_f.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model_f.add(Flatten())\n",
    "\n",
    "model_f.add(Dense(nb_dense[0], activation='relu'))\n",
    "model_f.add(Dropout(0.5))\n",
    "\n",
    "model_f.add(Dense(nb_dense[1], activation='relu'))\n",
    "model_f.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model_f.add(Dense(nb_dense[2], activation='softmax'))\n",
    "model_f.compile(optimizer=tf.keras.optimizers.Adadelta(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_f.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /Users/smalagi/Desktop/Project_data/train ; batch size = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/3462510936.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_f.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:19: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/67 [===========================>..] - ETA: 13s - loss: 3.4070 - categorical_accuracy: 0.1703Batch:  67 Index: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:44: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - ETA: 0s - loss: 3.4024 - categorical_accuracy: 0.1719Source path =  /Users/smalagi/Desktop/Project_data/val ; batch size = 10\n",
      "\n",
      "Epoch 1: saving model to model_init_2024-02-0310_09_19.822402/model-00001-3.40237-0.17195-1.60182-0.31000.h5\n",
      "67/67 [==============================] - 345s 5s/step - loss: 3.4024 - categorical_accuracy: 0.1719 - val_loss: 1.6018 - val_categorical_accuracy: 0.3100 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.2698 - categorical_accuracy: 0.2040\n",
      "Epoch 2: saving model to model_init_2024-02-0310_09_19.822402/model-00002-3.26983-0.20398-1.60892-0.30000.h5\n",
      "67/67 [==============================] - 142s 2s/step - loss: 3.2698 - categorical_accuracy: 0.2040 - val_loss: 1.6089 - val_categorical_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.2391 - categorical_accuracy: 0.1891\n",
      "Epoch 3: saving model to model_init_2024-02-0310_09_19.822402/model-00003-3.23911-0.18905-1.65107-0.26000.h5\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "67/67 [==============================] - 140s 2s/step - loss: 3.2391 - categorical_accuracy: 0.1891 - val_loss: 1.6511 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.6721 - categorical_accuracy: 0.2438\n",
      "Epoch 4: saving model to model_init_2024-02-0310_09_19.822402/model-00004-2.67211-0.24378-1.61597-0.24000.h5\n",
      "67/67 [==============================] - 139s 2s/step - loss: 2.6721 - categorical_accuracy: 0.2438 - val_loss: 1.6160 - val_categorical_accuracy: 0.2400 - lr: 5.0000e-04\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.1611 - categorical_accuracy: 0.1692\n",
      "Epoch 5: saving model to model_init_2024-02-0310_09_19.822402/model-00005-3.16112-0.16915-1.65198-0.27000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "67/67 [==============================] - 141s 2s/step - loss: 3.1611 - categorical_accuracy: 0.1692 - val_loss: 1.6520 - val_categorical_accuracy: 0.2700 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.0949 - categorical_accuracy: 0.1791\n",
      "Epoch 6: saving model to model_init_2024-02-0310_09_19.822402/model-00006-3.09492-0.17910-1.60823-0.27000.h5\n",
      "67/67 [==============================] - 142s 2s/step - loss: 3.0949 - categorical_accuracy: 0.1791 - val_loss: 1.6082 - val_categorical_accuracy: 0.2700 - lr: 2.5000e-04\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.9341 - categorical_accuracy: 0.2289\n",
      "Epoch 7: saving model to model_init_2024-02-0310_09_19.822402/model-00007-2.93411-0.22886-1.64673-0.24000.h5\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "67/67 [==============================] - 143s 2s/step - loss: 2.9341 - categorical_accuracy: 0.2289 - val_loss: 1.6467 - val_categorical_accuracy: 0.2400 - lr: 2.5000e-04\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.9508 - categorical_accuracy: 0.2239\n",
      "Epoch 8: saving model to model_init_2024-02-0310_09_19.822402/model-00008-2.95079-0.22388-1.63330-0.24000.h5\n",
      "67/67 [==============================] - 139s 2s/step - loss: 2.9508 - categorical_accuracy: 0.2239 - val_loss: 1.6333 - val_categorical_accuracy: 0.2400 - lr: 1.2500e-04\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.9307 - categorical_accuracy: 0.2139\n",
      "Epoch 9: saving model to model_init_2024-02-0310_09_19.822402/model-00009-2.93065-0.21393-1.69598-0.23000.h5\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "67/67 [==============================] - 142s 2s/step - loss: 2.9307 - categorical_accuracy: 0.2139 - val_loss: 1.6960 - val_categorical_accuracy: 0.2300 - lr: 1.2500e-04\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.9000 - categorical_accuracy: 0.2139\n",
      "Epoch 10: saving model to model_init_2024-02-0310_09_19.822402/model-00010-2.89996-0.21393-1.66774-0.28000.h5\n",
      "67/67 [==============================] - 140s 2s/step - loss: 2.9000 - categorical_accuracy: 0.2139 - val_loss: 1.6677 - val_categorical_accuracy: 0.2800 - lr: 6.2500e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff4e0ff5960>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "train_generator = generator_ex(train_path, train_doc, batch_size)\n",
    "val_generator = generator_ex(val_path, val_doc, batch_size)\n",
    "num_epochs = 10\n",
    "model_f.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_32 (Conv3D)          (None, 30, 120, 120, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 30, 120, 120, 8)   32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_32 (Activation)  (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " max_pooling3d_22 (MaxPooli  (None, 15, 60, 60, 8)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_33 (Conv3D)          (None, 15, 60, 60, 16)    3472      \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 15, 60, 60, 16)    64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_33 (Activation)  (None, 15, 60, 60, 16)    0         \n",
      "                                                                 \n",
      " max_pooling3d_23 (MaxPooli  (None, 7, 30, 30, 16)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_34 (Conv3D)          (None, 7, 30, 30, 32)     4640      \n",
      "                                                                 \n",
      " batch_normalization_16 (Ba  (None, 7, 30, 30, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_34 (Activation)  (None, 7, 30, 30, 32)     0         \n",
      "                                                                 \n",
      " max_pooling3d_24 (MaxPooli  (None, 3, 15, 15, 32)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_35 (Conv3D)          (None, 3, 15, 15, 64)     18496     \n",
      "                                                                 \n",
      " batch_normalization_17 (Ba  (None, 3, 15, 15, 64)     256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_35 (Activation)  (None, 3, 15, 15, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d_25 (MaxPooli  (None, 1, 7, 7, 64)       0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1000)              3137000   \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3667749 (13.99 MB)\n",
      "Trainable params: 3667509 (13.99 MB)\n",
      "Non-trainable params: 240 (960.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = 30 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height \n",
    "\n",
    "classes = 5\n",
    "channel = 3\n",
    "nb_dense = [1000, 500, 5]\n",
    "\n",
    "input_shape=(x,y,z,channel)\n",
    "\n",
    "# Define model\n",
    "model_g = Sequential()\n",
    "\n",
    "model_g.add(Conv3D(nb_filters[0], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "model_g.add(BatchNormalization())\n",
    "model_g.add(Activation('relu'))\n",
    "\n",
    "model_g.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_g.add(Conv3D(nb_filters[1], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model_g.add(BatchNormalization())\n",
    "model_g.add(Activation('relu'))\n",
    "\n",
    "model_g.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_g.add(Conv3D(nb_filters[2], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model_g.add(BatchNormalization())\n",
    "model_g.add(Activation('relu'))\n",
    "\n",
    "model_g.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_g.add(Conv3D(nb_filters[3], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model_g.add(BatchNormalization())\n",
    "model_g.add(Activation('relu'))\n",
    "\n",
    "model_g.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model_g.add(Flatten())\n",
    "\n",
    "model_g.add(Dense(nb_dense[0], activation='relu'))\n",
    "model_g.add(Dropout(0.5))\n",
    "\n",
    "model_g.add(Dense(nb_dense[1], activation='relu'))\n",
    "model_g.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model_g.add(Dense(nb_dense[2], activation='softmax'))\n",
    "model_g.compile(optimizer=tf.keras.optimizers.Adadelta(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /Users/smalagi/Desktop/Project_data/train ; batch size = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/2187855109.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_g.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:19: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/67 [===========================>..] - ETA: 12s - loss: 3.0206 - categorical_accuracy: 0.1859Batch:  67 Index: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:44: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - ETA: 0s - loss: 3.0366 - categorical_accuracy: 0.1840Source path =  /Users/smalagi/Desktop/Project_data/val ; batch size = 10\n",
      "\n",
      "Epoch 1: saving model to model_init_2024-02-0310_09_19.822402/model-00001-3.03656-0.18401-1.60694-0.28000.h5\n",
      "67/67 [==============================] - 330s 5s/step - loss: 3.0366 - categorical_accuracy: 0.1840 - val_loss: 1.6069 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.8482 - categorical_accuracy: 0.1990\n",
      "Epoch 2: saving model to model_init_2024-02-0310_09_19.822402/model-00002-2.84817-0.19900-1.60961-0.24000.h5\n",
      "67/67 [==============================] - 142s 2s/step - loss: 2.8482 - categorical_accuracy: 0.1990 - val_loss: 1.6096 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.7310 - categorical_accuracy: 0.2139\n",
      "Epoch 3: saving model to model_init_2024-02-0310_09_19.822402/model-00003-2.73101-0.21393-1.62750-0.26000.h5\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "67/67 [==============================] - 137s 2s/step - loss: 2.7310 - categorical_accuracy: 0.2139 - val_loss: 1.6275 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.7827 - categorical_accuracy: 0.2040\n",
      "Epoch 4: saving model to model_init_2024-02-0310_09_19.822402/model-00004-2.78275-0.20398-1.66879-0.26000.h5\n",
      "67/67 [==============================] - 136s 2s/step - loss: 2.7827 - categorical_accuracy: 0.2040 - val_loss: 1.6688 - val_categorical_accuracy: 0.2600 - lr: 5.0000e-04\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.6809 - categorical_accuracy: 0.1891\n",
      "Epoch 5: saving model to model_init_2024-02-0310_09_19.822402/model-00005-2.68091-0.18905-1.63552-0.18000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "67/67 [==============================] - 137s 2s/step - loss: 2.6809 - categorical_accuracy: 0.1891 - val_loss: 1.6355 - val_categorical_accuracy: 0.1800 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.7554 - categorical_accuracy: 0.1791\n",
      "Epoch 6: saving model to model_init_2024-02-0310_09_19.822402/model-00006-2.75541-0.17910-1.68508-0.16000.h5\n",
      "67/67 [==============================] - 138s 2s/step - loss: 2.7554 - categorical_accuracy: 0.1791 - val_loss: 1.6851 - val_categorical_accuracy: 0.1600 - lr: 2.5000e-04\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.6781 - categorical_accuracy: 0.2438\n",
      "Epoch 7: saving model to model_init_2024-02-0310_09_19.822402/model-00007-2.67812-0.24378-1.67099-0.17000.h5\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "67/67 [==============================] - 138s 2s/step - loss: 2.6781 - categorical_accuracy: 0.2438 - val_loss: 1.6710 - val_categorical_accuracy: 0.1700 - lr: 2.5000e-04\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.7299 - categorical_accuracy: 0.1990\n",
      "Epoch 8: saving model to model_init_2024-02-0310_09_19.822402/model-00008-2.72989-0.19900-1.73048-0.19000.h5\n",
      "67/67 [==============================] - 136s 2s/step - loss: 2.7299 - categorical_accuracy: 0.1990 - val_loss: 1.7305 - val_categorical_accuracy: 0.1900 - lr: 1.2500e-04\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.6215 - categorical_accuracy: 0.2289\n",
      "Epoch 9: saving model to model_init_2024-02-0310_09_19.822402/model-00009-2.62147-0.22886-1.65570-0.20000.h5\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "67/67 [==============================] - 140s 2s/step - loss: 2.6215 - categorical_accuracy: 0.2289 - val_loss: 1.6557 - val_categorical_accuracy: 0.2000 - lr: 1.2500e-04\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.7340 - categorical_accuracy: 0.1841\n",
      "Epoch 10: saving model to model_init_2024-02-0310_09_19.822402/model-00010-2.73402-0.18408-1.70368-0.19000.h5\n",
      "67/67 [==============================] - 139s 2s/step - loss: 2.7340 - categorical_accuracy: 0.1841 - val_loss: 1.7037 - val_categorical_accuracy: 0.1900 - lr: 6.2500e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff4820859c0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "train_generator = generator_ex(train_path, train_doc, batch_size)\n",
    "val_generator = generator_ex(val_path, val_doc, batch_size)\n",
    "num_epochs = 10\n",
    "model_g.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_36 (Conv3D)          (None, 30, 120, 120, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization_18 (Ba  (None, 30, 120, 120, 8)   32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_36 (Activation)  (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " max_pooling3d_26 (MaxPooli  (None, 15, 60, 60, 8)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_37 (Conv3D)          (None, 15, 60, 60, 16)    3472      \n",
      "                                                                 \n",
      " batch_normalization_19 (Ba  (None, 15, 60, 60, 16)    64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_37 (Activation)  (None, 15, 60, 60, 16)    0         \n",
      "                                                                 \n",
      " max_pooling3d_27 (MaxPooli  (None, 7, 30, 30, 16)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_38 (Conv3D)          (None, 7, 30, 30, 32)     4640      \n",
      "                                                                 \n",
      " batch_normalization_20 (Ba  (None, 7, 30, 30, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_38 (Activation)  (None, 7, 30, 30, 32)     0         \n",
      "                                                                 \n",
      " max_pooling3d_28 (MaxPooli  (None, 3, 15, 15, 32)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_39 (Conv3D)          (None, 3, 15, 15, 64)     18496     \n",
      "                                                                 \n",
      " batch_normalization_21 (Ba  (None, 3, 15, 15, 64)     256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_39 (Activation)  (None, 3, 15, 15, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d_29 (MaxPooli  (None, 1, 7, 7, 64)       0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1000)              3137000   \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3667749 (13.99 MB)\n",
      "Trainable params: 3667509 (13.99 MB)\n",
      "Non-trainable params: 240 (960.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = 30 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height \n",
    "\n",
    "classes = 5\n",
    "channel = 3\n",
    "\n",
    "input_shape=(x,y,z,channel)\n",
    "\n",
    "# Define model\n",
    "model_h = Sequential()\n",
    "\n",
    "model_h.add(Conv3D(nb_filters[0], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "model_h.add(BatchNormalization())\n",
    "model_h.add(Activation('relu'))\n",
    "\n",
    "model_h.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_h.add(Conv3D(nb_filters[1], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model_h.add(BatchNormalization())\n",
    "model_h.add(Activation('relu'))\n",
    "\n",
    "model_h.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_h.add(Conv3D(nb_filters[2], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model_h.add(BatchNormalization())\n",
    "model_h.add(Activation('relu'))\n",
    "\n",
    "model_h.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_h.add(Conv3D(nb_filters[3], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model_h.add(BatchNormalization())\n",
    "model_h.add(Activation('relu'))\n",
    "\n",
    "model_h.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model_h.add(Flatten())\n",
    "\n",
    "model_h.add(Dense(nb_dense[0], activation='relu'))\n",
    "model_h.add(Dropout(0.5))\n",
    "\n",
    "model_h.add(Dense(nb_dense[1], activation='relu'))\n",
    "model_h.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model_h.add(Dense(nb_dense[2], activation='softmax'))\n",
    "model_h.compile(optimizer=tf.keras.optimizers.Adadelta(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_h.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_filters = [8,16,32,64]\n",
    "nb_dense = [256, 128, 5]\n",
    "\n",
    "# Input\n",
    "input_shape=(30,120,120,3)\n",
    "\n",
    "# Define model\n",
    "model_final = Sequential()\n",
    "\n",
    "model_final.add(Conv3D(nb_filters[0], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "model_final.add(BatchNormalization())\n",
    "model_final.add(Activation('relu'))\n",
    "\n",
    "model_final.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_final.add(Conv3D(nb_filters[1], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model_final.add(BatchNormalization())\n",
    "model_final.add(Activation('relu'))\n",
    "\n",
    "model_final.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_final.add(Conv3D(nb_filters[2], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model_final.add(BatchNormalization())\n",
    "model_final.add(Activation('relu'))\n",
    "\n",
    "model_final.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_final.add(Conv3D(nb_filters[3], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model_final.add(Activation('relu'))\n",
    "model_final.add(Dropout(0.25))\n",
    "\n",
    "model_final.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model_final.add(Flatten())\n",
    "\n",
    "model_final.add(Dense(nb_dense[0], activation='relu'))\n",
    "model_final.add(Dropout(0.5))\n",
    "\n",
    "model_final.add(Dense(nb_dense[1], activation='relu'))\n",
    "model_final.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model_final.add(Dense(nb_dense[2], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_40 (Conv3D)          (None, 30, 120, 120, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization_22 (Ba  (None, 30, 120, 120, 8)   32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_40 (Activation)  (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " max_pooling3d_30 (MaxPooli  (None, 15, 60, 60, 8)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_41 (Conv3D)          (None, 15, 60, 60, 16)    3472      \n",
      "                                                                 \n",
      " batch_normalization_23 (Ba  (None, 15, 60, 60, 16)    64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_41 (Activation)  (None, 15, 60, 60, 16)    0         \n",
      "                                                                 \n",
      " max_pooling3d_31 (MaxPooli  (None, 7, 30, 30, 16)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_42 (Conv3D)          (None, 7, 30, 30, 32)     4640      \n",
      "                                                                 \n",
      " batch_normalization_24 (Ba  (None, 7, 30, 30, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_42 (Activation)  (None, 7, 30, 30, 32)     0         \n",
      "                                                                 \n",
      " max_pooling3d_32 (MaxPooli  (None, 3, 15, 15, 32)     0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " conv3d_43 (Conv3D)          (None, 3, 15, 15, 64)     18496     \n",
      "                                                                 \n",
      " activation_43 (Activation)  (None, 3, 15, 15, 64)     0         \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 3, 15, 15, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d_33 (MaxPooli  (None, 1, 7, 7, 64)       0         \n",
      " ng3D)                                                           \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 256)               803072    \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 864101 (3.30 MB)\n",
      "Trainable params: 863989 (3.30 MB)\n",
      "Non-trainable params: 112 (448.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = tf.keras.optimizers.Adam() #write your optimizer\n",
    "model_final.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_final.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator_ex(train_path, train_doc, batch_size)\n",
    "val_generator = generator_ex(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /Users/smalagi/Desktop/Project_data/train ; batch size = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/655006215.py:3: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_final.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:19: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "64/67 [===========================>..] - ETA: 12s - loss: 2.0349 - categorical_accuracy: 0.2547Batch:  67 Index: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:44: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - ETA: 0s - loss: 2.0226 - categorical_accuracy: 0.2489Source path =  /Users/smalagi/Desktop/Project_data/val ; batch size = 10\n",
      "\n",
      "Epoch 1: saving model to model_init_2024-02-0310_09_19.822402/model-00001-2.02263-0.24887-1.54481-0.43000.h5\n",
      "67/67 [==============================] - 327s 5s/step - loss: 2.0226 - categorical_accuracy: 0.2489 - val_loss: 1.5448 - val_categorical_accuracy: 0.4300 - lr: 0.0010\n",
      "Epoch 2/12\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5048 - categorical_accuracy: 0.3134\n",
      "Epoch 2: saving model to model_init_2024-02-0310_09_19.822402/model-00002-1.50479-0.31343-1.62629-0.18000.h5\n",
      "67/67 [==============================] - 137s 2s/step - loss: 1.5048 - categorical_accuracy: 0.3134 - val_loss: 1.6263 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 3/12\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5532 - categorical_accuracy: 0.2736\n",
      "Epoch 3: saving model to model_init_2024-02-0310_09_19.822402/model-00003-1.55321-0.27363-1.70301-0.16000.h5\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "67/67 [==============================] - 135s 2s/step - loss: 1.5532 - categorical_accuracy: 0.2736 - val_loss: 1.7030 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 4/12\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4773 - categorical_accuracy: 0.3134\n",
      "Epoch 4: saving model to model_init_2024-02-0310_09_19.822402/model-00004-1.47726-0.31343-1.50442-0.33000.h5\n",
      "67/67 [==============================] - 137s 2s/step - loss: 1.4773 - categorical_accuracy: 0.3134 - val_loss: 1.5044 - val_categorical_accuracy: 0.3300 - lr: 5.0000e-04\n",
      "Epoch 5/12\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4434 - categorical_accuracy: 0.3881\n",
      "Epoch 5: saving model to model_init_2024-02-0310_09_19.822402/model-00005-1.44341-0.38806-1.34440-0.54000.h5\n",
      "67/67 [==============================] - 135s 2s/step - loss: 1.4434 - categorical_accuracy: 0.3881 - val_loss: 1.3444 - val_categorical_accuracy: 0.5400 - lr: 5.0000e-04\n",
      "Epoch 6/12\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.3095 - categorical_accuracy: 0.4328\n",
      "Epoch 6: saving model to model_init_2024-02-0310_09_19.822402/model-00006-1.30954-0.43284-1.28141-0.55000.h5\n",
      "67/67 [==============================] - 134s 2s/step - loss: 1.3095 - categorical_accuracy: 0.4328 - val_loss: 1.2814 - val_categorical_accuracy: 0.5500 - lr: 5.0000e-04\n",
      "Epoch 7/12\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4904 - categorical_accuracy: 0.3333\n",
      "Epoch 7: saving model to model_init_2024-02-0310_09_19.822402/model-00007-1.49037-0.33333-1.31438-0.51000.h5\n",
      "67/67 [==============================] - 136s 2s/step - loss: 1.4904 - categorical_accuracy: 0.3333 - val_loss: 1.3144 - val_categorical_accuracy: 0.5100 - lr: 5.0000e-04\n",
      "Epoch 8/12\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.3501 - categorical_accuracy: 0.4279\n",
      "Epoch 8: saving model to model_init_2024-02-0310_09_19.822402/model-00008-1.35009-0.42786-1.24118-0.42000.h5\n",
      "67/67 [==============================] - 137s 2s/step - loss: 1.3501 - categorical_accuracy: 0.4279 - val_loss: 1.2412 - val_categorical_accuracy: 0.4200 - lr: 5.0000e-04\n",
      "Epoch 9/12\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.3700 - categorical_accuracy: 0.4279\n",
      "Epoch 9: saving model to model_init_2024-02-0310_09_19.822402/model-00009-1.37000-0.42786-1.22602-0.61000.h5\n",
      "67/67 [==============================] - 138s 2s/step - loss: 1.3700 - categorical_accuracy: 0.4279 - val_loss: 1.2260 - val_categorical_accuracy: 0.6100 - lr: 5.0000e-04\n",
      "Epoch 10/12\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.2069 - categorical_accuracy: 0.4826\n",
      "Epoch 10: saving model to model_init_2024-02-0310_09_19.822402/model-00010-1.20695-0.48259-1.03137-0.63000.h5\n",
      "67/67 [==============================] - 136s 2s/step - loss: 1.2069 - categorical_accuracy: 0.4826 - val_loss: 1.0314 - val_categorical_accuracy: 0.6300 - lr: 5.0000e-04\n",
      "Epoch 11/12\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.3058 - categorical_accuracy: 0.4677\n",
      "Epoch 11: saving model to model_init_2024-02-0310_09_19.822402/model-00011-1.30580-0.46766-1.21020-0.65000.h5\n",
      "67/67 [==============================] - 138s 2s/step - loss: 1.3058 - categorical_accuracy: 0.4677 - val_loss: 1.2102 - val_categorical_accuracy: 0.6500 - lr: 5.0000e-04\n",
      "Epoch 12/12\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.2201 - categorical_accuracy: 0.4876\n",
      "Epoch 12: saving model to model_init_2024-02-0310_09_19.822402/model-00012-1.22014-0.48756-1.05690-0.58000.h5\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "67/67 [==============================] - 135s 2s/step - loss: 1.2201 - categorical_accuracy: 0.4876 - val_loss: 1.0569 - val_categorical_accuracy: 0.5800 - lr: 5.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fec88519030>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "num_epochs = 12\n",
    "model_final.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/3909543589.py:3: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_final.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
      "/var/folders/v6/6bvfh58j79sf_23ln412v_fr0000gq/T/ipykernel_26161/157304017.py:19: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.0905 - categorical_accuracy: 0.5672\n",
      "Epoch 1: saving model to model_init_2024-02-0310_09_19.822402/model-00001-1.09050-0.56716-0.99309-0.64000.h5\n",
      "67/67 [==============================] - 137s 2s/step - loss: 1.0905 - categorical_accuracy: 0.5672 - val_loss: 0.9931 - val_categorical_accuracy: 0.6400 - lr: 2.5000e-04\n",
      "Epoch 2/14\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.0675 - categorical_accuracy: 0.5721\n",
      "Epoch 2: saving model to model_init_2024-02-0310_09_19.822402/model-00002-1.06747-0.57214-0.98272-0.58000.h5\n",
      "67/67 [==============================] - 135s 2s/step - loss: 1.0675 - categorical_accuracy: 0.5721 - val_loss: 0.9827 - val_categorical_accuracy: 0.5800 - lr: 2.5000e-04\n",
      "Epoch 3/14\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.0469 - categorical_accuracy: 0.5721\n",
      "Epoch 3: saving model to model_init_2024-02-0310_09_19.822402/model-00003-1.04687-0.57214-0.89766-0.68000.h5\n",
      "67/67 [==============================] - 134s 2s/step - loss: 1.0469 - categorical_accuracy: 0.5721 - val_loss: 0.8977 - val_categorical_accuracy: 0.6800 - lr: 2.5000e-04\n",
      "Epoch 4/14\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.0340 - categorical_accuracy: 0.5920\n",
      "Epoch 4: saving model to model_init_2024-02-0310_09_19.822402/model-00004-1.03403-0.59204-1.00686-0.62000.h5\n",
      "67/67 [==============================] - 135s 2s/step - loss: 1.0340 - categorical_accuracy: 0.5920 - val_loss: 1.0069 - val_categorical_accuracy: 0.6200 - lr: 2.5000e-04\n",
      "Epoch 5/14\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.0015 - categorical_accuracy: 0.5970\n",
      "Epoch 5: saving model to model_init_2024-02-0310_09_19.822402/model-00005-1.00153-0.59701-0.82853-0.68000.h5\n",
      "67/67 [==============================] - 135s 2s/step - loss: 1.0015 - categorical_accuracy: 0.5970 - val_loss: 0.8285 - val_categorical_accuracy: 0.6800 - lr: 2.5000e-04\n",
      "Epoch 6/14\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.9135 - categorical_accuracy: 0.6418\n",
      "Epoch 6: saving model to model_init_2024-02-0310_09_19.822402/model-00006-0.91348-0.64179-0.84582-0.71000.h5\n",
      "67/67 [==============================] - 137s 2s/step - loss: 0.9135 - categorical_accuracy: 0.6418 - val_loss: 0.8458 - val_categorical_accuracy: 0.7100 - lr: 2.5000e-04\n",
      "Epoch 7/14\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.9263 - categorical_accuracy: 0.6169\n",
      "Epoch 7: saving model to model_init_2024-02-0310_09_19.822402/model-00007-0.92631-0.61692-0.86447-0.70000.h5\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "67/67 [==============================] - 133s 2s/step - loss: 0.9263 - categorical_accuracy: 0.6169 - val_loss: 0.8645 - val_categorical_accuracy: 0.7000 - lr: 2.5000e-04\n",
      "Epoch 8/14\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.9076 - categorical_accuracy: 0.6667\n",
      "Epoch 8: saving model to model_init_2024-02-0310_09_19.822402/model-00008-0.90762-0.66667-0.82345-0.70000.h5\n",
      "67/67 [==============================] - 135s 2s/step - loss: 0.9076 - categorical_accuracy: 0.6667 - val_loss: 0.8234 - val_categorical_accuracy: 0.7000 - lr: 1.2500e-04\n",
      "Epoch 9/14\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.9180 - categorical_accuracy: 0.6318\n",
      "Epoch 9: saving model to model_init_2024-02-0310_09_19.822402/model-00009-0.91803-0.63184-0.75410-0.72000.h5\n",
      "67/67 [==============================] - 134s 2s/step - loss: 0.9180 - categorical_accuracy: 0.6318 - val_loss: 0.7541 - val_categorical_accuracy: 0.7200 - lr: 1.2500e-04\n",
      "Epoch 10/14\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.7291 - categorical_accuracy: 0.7065\n",
      "Epoch 10: saving model to model_init_2024-02-0310_09_19.822402/model-00010-0.72911-0.70647-0.77767-0.66000.h5\n",
      "67/67 [==============================] - 135s 2s/step - loss: 0.7291 - categorical_accuracy: 0.7065 - val_loss: 0.7777 - val_categorical_accuracy: 0.6600 - lr: 1.2500e-04\n",
      "Epoch 11/14\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.8011 - categorical_accuracy: 0.7065\n",
      "Epoch 11: saving model to model_init_2024-02-0310_09_19.822402/model-00011-0.80113-0.70647-0.71607-0.72000.h5\n",
      "67/67 [==============================] - 136s 2s/step - loss: 0.8011 - categorical_accuracy: 0.7065 - val_loss: 0.7161 - val_categorical_accuracy: 0.7200 - lr: 1.2500e-04\n",
      "Epoch 12/14\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.9063 - categorical_accuracy: 0.6716\n",
      "Epoch 12: saving model to model_init_2024-02-0310_09_19.822402/model-00012-0.90635-0.67164-0.72202-0.73000.h5\n",
      "67/67 [==============================] - 137s 2s/step - loss: 0.9063 - categorical_accuracy: 0.6716 - val_loss: 0.7220 - val_categorical_accuracy: 0.7300 - lr: 1.2500e-04\n",
      "Epoch 13/14\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.7282 - categorical_accuracy: 0.6965\n",
      "Epoch 13: saving model to model_init_2024-02-0310_09_19.822402/model-00013-0.72816-0.69652-0.76047-0.70000.h5\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "67/67 [==============================] - 137s 2s/step - loss: 0.7282 - categorical_accuracy: 0.6965 - val_loss: 0.7605 - val_categorical_accuracy: 0.7000 - lr: 1.2500e-04\n",
      "Epoch 14/14\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.7276 - categorical_accuracy: 0.7562\n",
      "Epoch 14: saving model to model_init_2024-02-0310_09_19.822402/model-00014-0.72762-0.75622-0.63232-0.80000.h5\n",
      "67/67 [==============================] - 138s 2s/step - loss: 0.7276 - categorical_accuracy: 0.7562 - val_loss: 0.6323 - val_categorical_accuracy: 0.8000 - lr: 6.2500e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fec885c5ea0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 12\n",
    "num_epochs = 14\n",
    "model_final.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model Selection\n",
    "The model from Epoch 14 is identified as the best-fit model for deployment, highlighted by its exceptional performance metrics:\n",
    "\n",
    "- **Highest Validation Accuracy:**\n",
    "  - Achieved a validation accuracy of 80.00%, indicating a strong ability of the model to generalize well to new, unseen data.\n",
    "\n",
    "- **Lowest Validation Loss:**\n",
    "  - Recorded a validation loss of 0.6323, suggesting the model has a good understanding of the data with minimal error in its predictions.\n",
    "\n",
    "## Conclusion\n",
    "Based on the comprehensive evaluation, the model trained up to Epoch 14 (`model-00014-0.72762-0.75622-0.63232-0.80000.h5`) is highly recommended for deployment. This recommendation is based on several key factors:\n",
    "\n",
    "- Provides the highest validation accuracy among all models evaluated, underlining its superior capability to correctly interpret and classify gesture data.\n",
    "- Demonstrates effective learning and generalization capability, crucial for real-world applications where the model will encounter diverse and previously unseen gesture data.\n",
    "- Strikes an optimal balance between learning efficiency and performance, considering the inherent complexity and variability of gesture data. This balance ensures that the model is not only accurate but also practical for real-time applications in gesture recognition.\n",
    "\n",
    "This model represents a significant step forward in developing intuitive and user-friendly interaction methods for smart TV applications, leveraging the latest advancements in 3D convolutional neural networks and machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
